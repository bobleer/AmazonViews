{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-23T19:21:24.106764Z",
     "start_time": "2019-07-23T19:21:22.027760Z"
    }
   },
   "outputs": [],
   "source": [
    "# Packages\n",
    "from time import time\n",
    "import re\n",
    "import os\n",
    "import nltk\n",
    "import numpy as np\n",
    "import multiprocessing as mp\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"Progress bar\")\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "import multiprocessing as mp\n",
    "import pickle\n",
    "\n",
    "cores = mp.cpu_count()\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "Tokenizer = TweetTokenizer()\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from emoji import demojize\n",
    "\n",
    "# Plot settings\n",
    "sns.set_context('notebook') \n",
    "sns.set_style('ticks') \n",
    "colours = ['#1F77B4', '#FF7F0E', '#2CA02C', '#DB2728', '#9467BD', '#8C564B', '#E377C2','#7F7F7F', '#BCBD22', '#17BECF']\n",
    "crayon = ['#4E79A7','#F28E2C','#E15759','#76B7B2','#59A14F', '#EDC949','#AF7AA1','#FF9DA7','#9C755F','#BAB0AB']\n",
    "sns.set_palette(colours)\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (9, 6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-23T19:24:45.854986Z",
     "start_time": "2019-07-23T19:24:45.845935Z"
    }
   },
   "outputs": [],
   "source": [
    "def _apply_df(args):\n",
    "    df, func, kwargs = args\n",
    "    return df.progress_apply(func, **kwargs)\n",
    "\n",
    "def multi_apply(df, func, **kwargs):\n",
    "    workers = kwargs.pop('workers')\n",
    "    pool = mp.Pool(processes=workers)\n",
    "    result = pool.map(_apply_df, [(d, func, kwargs) for d in np.array_split(df, workers)])\n",
    "    pool.close()\n",
    "    return pd.concat(list(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv', header=None)\n",
    "\n",
    "# Merge title and content\n",
    "train['Text'] = train[1]+' '+train[2]\n",
    "train = train.drop(columns=[1,2])\n",
    "\n",
    "# Negative = 0, Positive = 1\n",
    "train[0] = train[0].map(lambda x: x-1)\n",
    "train.rename(columns={0:'Sentiment'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('test.csv', header=None)\n",
    "\n",
    "# Merge title and content\n",
    "test['Text'] = test[1]+' '+test[2]\n",
    "test = test.drop(columns=[1,2])\n",
    "\n",
    "# Negative = 0, Positive = 1\n",
    "test[0] = test[0].map(lambda x: x-1)\n",
    "test.rename(columns={0:'Sentiment'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge dataset\n",
    "fullset = pd.concat([train,test], axis=0, ignore_index=True)\n",
    "del(train)\n",
    "del(test)\n",
    "fullset.to_csv('fullset.csv', index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-23T11:26:08.439424Z",
     "start_time": "2019-07-23T11:25:50.196387Z"
    }
   },
   "outputs": [],
   "source": [
    "fullset = pd.read_csv('fullset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-23T06:07:35.653645Z",
     "start_time": "2019-07-23T06:07:35.646127Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3999995</th>\n",
       "      <td>0</td>\n",
       "      <td>Unbelievable- In a Bad Way We bought this Thom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999996</th>\n",
       "      <td>0</td>\n",
       "      <td>Almost Great, Until it Broke... My son recieve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999997</th>\n",
       "      <td>0</td>\n",
       "      <td>Disappointed !!! I bought this toy for my son ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999998</th>\n",
       "      <td>1</td>\n",
       "      <td>Classic Jessica Mitford This is a compilation ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999999</th>\n",
       "      <td>0</td>\n",
       "      <td>Comedy Scene, and Not Heard This DVD will be a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Sentiment                                               Text\n",
       "3999995          0  Unbelievable- In a Bad Way We bought this Thom...\n",
       "3999996          0  Almost Great, Until it Broke... My son recieve...\n",
       "3999997          0  Disappointed !!! I bought this toy for my son ...\n",
       "3999998          1  Classic Jessica Mitford This is a compilation ...\n",
       "3999999          0  Comedy Scene, and Not Heard This DVD will be a..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fullset.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-23T06:07:35.738285Z",
     "start_time": "2019-07-23T06:07:35.686252Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    2000000\n",
       "0    2000000\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fullset['Sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-23T11:26:09.818186Z",
     "start_time": "2019-07-23T11:26:08.475277Z"
    }
   },
   "outputs": [],
   "source": [
    "fullset['Text'] = fullset['Text'].apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-23T06:07:38.821560Z",
     "start_time": "2019-07-23T06:07:37.135285Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    4000000.0\n",
       "mean         431.0\n",
       "std          238.0\n",
       "min            3.0\n",
       "50%          382.0\n",
       "95%          894.0\n",
       "99%          988.0\n",
       "max         1014.0\n",
       "Name: Text, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(fullset['Text'].apply(len)).describe(percentiles=[.95, .99]).round(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text lowercase + Stemming\n",
    "def pre_proc(text):\n",
    "    return ''.join([PorterStemmer().stem(x) for x in Tokenizer.tokenize(text.lower()) if x != ''])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress bar: 100%|██████████| 571428/571428 [53:35<00:00, 177.72it/s]  \n",
      "Progress bar: 100%|██████████| 571428/571428 [53:53<00:00, 176.70it/s]\n",
      "Progress bar: 100%|██████████| 571429/571429 [54:11<00:00, 175.73it/s]\n",
      "Progress bar: 100%|██████████| 571428/571428 [54:45<00:00, 173.95it/s]\n",
      "Progress bar: 100%|██████████| 571429/571429 [54:50<00:00, 173.65it/s]\n",
      "Progress bar: 100%|██████████| 571429/571429 [55:10<00:00, 172.60it/s]\n",
      "Progress bar: 100%|██████████| 571429/571429 [55:31<00:00, 171.52it/s]\n"
     ]
    }
   ],
   "source": [
    "fullset['Token'] = multi_apply(fullset['Text'], pre_proc, workers=cores-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-23T19:22:07.939387Z",
     "start_time": "2019-07-23T19:22:04.774297Z"
    }
   },
   "outputs": [],
   "source": [
    "fullset = pd.read_csv('fullset_ez_processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-23T06:22:25.408953Z",
     "start_time": "2019-07-23T06:22:25.405734Z"
    }
   },
   "outputs": [],
   "source": [
    "fdist = nltk.FreqDist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-23T06:27:44.091058Z",
     "start_time": "2019-07-23T06:22:26.076378Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in fullset['Token']:\n",
    "    for word in i.split():\n",
    "        fdist[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-23T06:38:18.409568Z",
     "start_time": "2019-07-23T06:38:15.842029Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     1804633.0\n",
       "mean          175.0\n",
       "std         21403.0\n",
       "min             1.0\n",
       "50%             1.0\n",
       "95%            17.0\n",
       "99%           271.0\n",
       "max      15792716.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count the number of UNIQUE word\n",
    "features = pd.Series(dict(fdist))\n",
    "features.describe(percentiles=[.95, .99]).round(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-23T06:45:33.109686Z",
     "start_time": "2019-07-23T06:45:33.051521Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1246917 words which only appear once.\n"
     ]
    }
   ],
   "source": [
    "features_1 = features[features==1]\n",
    "print('There are',len(features_1),'features which only appear once.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-23T07:24:25.352661Z",
     "start_time": "2019-07-23T07:24:25.348440Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some once features are like this: 'peaceful.on'\n",
      "So need to re-split them.\n"
     ]
    }
   ],
   "source": [
    "print('Some once features are like this:','\\''+features_1.index[1]+'\\'')\n",
    "print('So need to re-split them.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-23T09:45:43.765784Z",
     "start_time": "2019-07-23T09:45:43.679005Z"
    }
   },
   "outputs": [],
   "source": [
    "features_re = features[features<=1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-23T09:45:56.312619Z",
     "start_time": "2019-07-23T09:45:56.024765Z"
    }
   },
   "outputs": [],
   "source": [
    "relist = [x for x in features_1.index if (not x.isalpha())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-23T09:45:57.167370Z",
     "start_time": "2019-07-23T09:45:57.162816Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 824988 features with punctuations.\n"
     ]
    }
   ],
   "source": [
    "print('There are',len(relist),'features with punctuations.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-23T09:46:05.940756Z",
     "start_time": "2019-07-23T09:46:05.879325Z"
    }
   },
   "outputs": [],
   "source": [
    "relist_str = ''.join(relist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-23T09:55:36.503859Z",
     "start_time": "2019-07-23T09:55:35.579512Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎥|̣|∂|😁|„|'||7|6|̇|~|-|😠|😔|]|≠|☆|#|😍|/|[|9|【|.|_|😩|↓|5|₤|\u0007|)|⊖|👏|♠|8|💜|1|∅|℉|😡|&|\u0015|␟|╚|👎|ً|🎉|$|💅|（|💖|⌫|:|😀|}|,|⟨|\u0017|😎|0|=|>|╝|\"|☼|%|<|♡|⊕|;|】|4|′|\u0002|+|?|†||^|⊂|\\|）|‼|{|!|2|😉|̄|@|*|─|\u0016|♣|||(|3 \n",
      "\n",
      ":movie_camera:|̣|∂|:beaming_face_with_smiling_eyes:|„|'||7|6|̇|~|-|:angry_face:|:pensive_face:|]|≠|☆|#|:smiling_face_with_heart-eyes:|/|[|9|【|.|_|:weary_face:|↓|5|₤|\u0007|)|⊖|:clapping_hands:|:spade_suit:|8|:purple_heart:|1|∅|℉|:pouting_face:|&|\u0015|␟|╚|:thumbs_down:|ً|:party_popper:|$|:nail_polish:|（|:sparkling_heart:|⌫|:|:grinning_face:|}|,|⟨|\u0017|:smiling_face_with_sunglasses:|0|=|>|╝|\"|☼|%|<|♡|⊕|;|】|4|′|\u0002|+|?|†||^|⊂|\\|）|:double_exclamation_mark:|{|!|2|:winking_face:|̄|@|*|─|\u0016|:club_suit:|||(|3\n"
     ]
    }
   ],
   "source": [
    "separator = '|'.join(list(set([r'{}'.format(x) for x in relist_str if not x.isalpha()])))\n",
    "print(separator,'\\n')\n",
    "print(demojize(separator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-23T19:22:37.242092Z",
     "start_time": "2019-07-23T19:22:37.238068Z"
    }
   },
   "outputs": [],
   "source": [
    "def resplit(text):\n",
    "    # Translate emojis\n",
    "    text = demojize(text)\n",
    "    # Remove punctuation\n",
    "    for i in string.punctuation:\n",
    "        text = text.replace(i,' ')\n",
    "    # Token\n",
    "    text = Tokenizer.tokenize(text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullset.Token = fullset.Token.apply(resplit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3999995</th>\n",
       "      <td>0</td>\n",
       "      <td>[unbeliev, in, a, bad, way, we, bought, thi, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999996</th>\n",
       "      <td>0</td>\n",
       "      <td>[almost, great, until, it, broke, my, son, rec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999997</th>\n",
       "      <td>0</td>\n",
       "      <td>[disappoint, i, bought, thi, toy, for, my, son...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999998</th>\n",
       "      <td>1</td>\n",
       "      <td>[classic, jessica, mitford, thi, is, a, compil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999999</th>\n",
       "      <td>0</td>\n",
       "      <td>[comedi, scene, and, not, heard, thi, dvd, wil...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Sentiment                                              Token\n",
       "3999995          0  [unbeliev, in, a, bad, way, we, bought, thi, t...\n",
       "3999996          0  [almost, great, until, it, broke, my, son, rec...\n",
       "3999997          0  [disappoint, i, bought, thi, toy, for, my, son...\n",
       "3999998          1  [classic, jessica, mitford, thi, is, a, compil...\n",
       "3999999          0  [comedi, scene, and, not, heard, thi, dvd, wil..."
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fullset.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('fullset_resplit.pickle', 'wb') as f:\n",
    "    pickle.dump(fullset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-07-23T12:11:34.298Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4000000/4000000 [06:07<00:00, 10893.40it/s]\n"
     ]
    }
   ],
   "source": [
    "# Re-count\n",
    "fdist = nltk.FreqDist()\n",
    "\n",
    "for i in tqdm(fullset['Token']):\n",
    "    for word in i:\n",
    "        fdist[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'the': 15809493, 'i': 9294446, 'and': 8586104, 'a': 8061406, 'to': 7712911, 'it': 7578653, 'of': 6298197, 'thi': 5911844, 'is': 5530069, 'in': 3713530, ...})"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-07-23T12:11:35.332Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count      858011.0\n",
       "mean          375.0\n",
       "std         31793.0\n",
       "min             1.0\n",
       "50%             1.0\n",
       "95%            51.0\n",
       "99%          1065.0\n",
       "max      15809493.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count the number of UNIQUE word\n",
    "features = pd.Series(dict(fdist))\n",
    "features.describe(percentiles=[.95, .99]).round(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-07-23T12:11:36.529Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 47820 features which only appear once.\n"
     ]
    }
   ],
   "source": [
    "features_1 = features[features==3]\n",
    "print('There are',len(features_1),'features which appear only once.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-07-23T12:11:37.276Z"
    }
   },
   "outputs": [],
   "source": [
    "def rmonce(token):\n",
    "    return [x for x in token if x not in features_1.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-07-23T12:11:37.903Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress bar: 100%|██████████| 4000000/4000000 [14:54<00:00, 4471.89it/s]   \n"
     ]
    }
   ],
   "source": [
    "# Remove words which appear only once.\n",
    "fullset.Token = fullset.Token.progress_apply(rmonce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    4000000.0\n",
       "mean          80.0\n",
       "std           44.0\n",
       "min            0.0\n",
       "50%           72.0\n",
       "95%          165.0\n",
       "99%          186.0\n",
       "max          257.0\n",
       "Name: Token, dtype: float64"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count the length of sentences\n",
    "(fullset.Token.apply(len)).describe(percentiles=[.95, .99]).round(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullset_original = pd.read_csv('fullset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "294435     ........ ............ ..... ..... ...... ........\n",
      "3584048    -_- ' ' '''' '''' '' '' ''' '''''? '' '' ' '' ...\n",
      "Name: Text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(fullset_original.Text[fullset.Token.apply(len)==0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullset = fullset[fullset.Token.apply(len)!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    3999998.0\n",
       "mean          80.0\n",
       "std           44.0\n",
       "min            1.0\n",
       "50%           72.0\n",
       "95%          165.0\n",
       "99%          186.0\n",
       "max          257.0\n",
       "Name: Token, dtype: float64"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(fullset.Token.apply(len)).describe(percentiles=[.95, .99]).round(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('fullset_resplit_rmonce.pickle', 'wb') as f:\n",
    "    pickle.dump(fullset, f, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 15809493),\n",
       " ('i', 9294446),\n",
       " ('and', 8586104),\n",
       " ('a', 8061406),\n",
       " ('to', 7712911),\n",
       " ('it', 7578653),\n",
       " ('of', 6298197),\n",
       " ('thi', 5911844),\n",
       " ('is', 5530069),\n",
       " ('in', 3713530),\n",
       " ('for', 3524304),\n",
       " ('that', 3245780),\n",
       " ('you', 2776538),\n",
       " ('wa', 2680975),\n",
       " ('not', 2612517),\n",
       " ('book', 2498410),\n",
       " ('but', 2345642),\n",
       " ('with', 2308551),\n",
       " ('on', 2281343),\n",
       " ('have', 2190331)]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The most frequent 20 words\n",
    "fdist.most_common()[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwordls = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmstopword(token):\n",
    "    return [x for x in token if x not in stopwordls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stopwords\n",
    "fullset.Token = fullset.Token.apply(rmstopword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-count\n",
    "fdist = nltk.FreqDist()\n",
    "\n",
    "for i in fullset['Token']:\n",
    "    for word in i:\n",
    "        fdist[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('thi', 5911844),\n",
       " ('wa', 2680975),\n",
       " ('book', 2498410),\n",
       " ('one', 1590682),\n",
       " ('like', 1289538),\n",
       " ('great', 1201557),\n",
       " ('veri', 1183127),\n",
       " ('good', 1167851),\n",
       " ('read', 1079779),\n",
       " ('use', 1002323),\n",
       " ('get', 995575),\n",
       " ('time', 944117),\n",
       " ('would', 939644),\n",
       " ('work', 875548),\n",
       " ('ha', 868094),\n",
       " ('movi', 773386),\n",
       " ('love', 772790),\n",
       " ('onli', 714689),\n",
       " ('hi', 665482),\n",
       " ('realli', 639812)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The most frequent 20 words\n",
    "fdist.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "810038"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many UNIQUE words\n",
    "len(fdist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    3999998.0\n",
       "mean          44.0\n",
       "std           24.0\n",
       "min            1.0\n",
       "50%           39.0\n",
       "95%           90.0\n",
       "99%          102.0\n",
       "max          212.0\n",
       "Name: Token, dtype: float64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(fullset.Token.apply(len)).describe(percentiles=[.95, .99]).round(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most frequent 55 words have 25% portion.\n",
      "The most frequent 294 words have 50% portion.\n",
      "The most frequent 1263 words have 75% portion.\n",
      "The most frequent 10323 words have 95% portion.\n",
      "The most frequent 68782 words have 99% portion.\n"
     ]
    }
   ],
   "source": [
    "fq25, fq50, fq75, fq95, fq99 = 0, 0, 0, 0, 0\n",
    "count = 0\n",
    "for i in fdist_df[0]:\n",
    "    fq25 += fdist.freq(i)\n",
    "    fq50 += fdist.freq(i)\n",
    "    fq75 += fdist.freq(i)\n",
    "    fq95 += fdist.freq(i)\n",
    "    fq99 += fdist.freq(i)\n",
    "    count += 1\n",
    "    if fq25 > 0.25:\n",
    "        print('The most frequent',count, 'words have 25% portion.')\n",
    "        fq25 -= 1\n",
    "    if fq50 > 0.50:\n",
    "        print('The most frequent',count, 'words have 50% portion.')\n",
    "        fq50 -= 1\n",
    "    if fq75 > 0.75:\n",
    "        print('The most frequent',count, 'words have 75% portion.')\n",
    "        fq75 -= 1\n",
    "    if fq95 > 0.95:\n",
    "        print('The most frequent',count, 'words have 95% portion.')\n",
    "        fq95 -= 1\n",
    "    if fq99 > 0.99:\n",
    "        print('The most frequent',count, 'words have 99% portion.')\n",
    "        fq99 -= 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('fullset_resplit_rmonce_nostopword.pickle', 'wb') as f:\n",
    "    pickle.dump(fullset, f, -1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
