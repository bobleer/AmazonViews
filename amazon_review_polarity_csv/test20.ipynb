{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-23T19:21:24.106764Z",
     "start_time": "2019-07-23T19:21:22.027760Z"
    }
   },
   "outputs": [],
   "source": [
    "# Packages\n",
    "from time import time\n",
    "import re\n",
    "import os\n",
    "import nltk\n",
    "import numpy as np\n",
    "import multiprocessing as mp\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"Progress bar\")\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "import multiprocessing as mp\n",
    "import pickle\n",
    "\n",
    "cores = mp.cpu_count()\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "Tokenizer = TweetTokenizer()\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from emoji import demojize\n",
    "\n",
    "# Plot settings\n",
    "sns.set_context('notebook') \n",
    "sns.set_style('ticks') \n",
    "colours = ['#1F77B4', '#FF7F0E', '#2CA02C', '#DB2728', '#9467BD', '#8C564B', '#E377C2','#7F7F7F', '#BCBD22', '#17BECF']\n",
    "crayon = ['#4E79A7','#F28E2C','#E15759','#76B7B2','#59A14F', '#EDC949','#AF7AA1','#FF9DA7','#9C755F','#BAB0AB']\n",
    "sns.set_palette(colours)\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (9, 6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-23T19:24:45.854986Z",
     "start_time": "2019-07-23T19:24:45.845935Z"
    }
   },
   "outputs": [],
   "source": [
    "def _apply_df(args):\n",
    "    df, func, kwargs = args\n",
    "    return df.progress_apply(func, **kwargs)\n",
    "\n",
    "def multi_apply(df, func, **kwargs):\n",
    "    workers = kwargs.pop('workers')\n",
    "    pool = mp.Pool(processes=workers)\n",
    "    result = pool.map(_apply_df, [(d, func, kwargs) for d in np.array_split(df, workers)])\n",
    "    pool.close()\n",
    "    return pd.concat(list(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv', header=None)\n",
    "\n",
    "# Merge title and content\n",
    "train['Text'] = train[1]+' '+train[2]\n",
    "train = train.drop(columns=[1,2])\n",
    "\n",
    "# Negative = 0, Positive = 1\n",
    "train[0] = train[0].map(lambda x: x-1)\n",
    "train.rename(columns={0:'Sentiment'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('test.csv', header=None)\n",
    "\n",
    "# Merge title and content\n",
    "test['Text'] = test[1]+' '+test[2]\n",
    "test = test.drop(columns=[1,2])\n",
    "\n",
    "# Negative = 0, Positive = 1\n",
    "test[0] = test[0].map(lambda x: x-1)\n",
    "test.rename(columns={0:'Sentiment'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge dataset\n",
    "fullset = pd.concat([train,test], axis=0, ignore_index=True)\n",
    "del(train)\n",
    "del(test)\n",
    "fullset.to_csv('fullset.csv', index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-23T11:26:08.439424Z",
     "start_time": "2019-07-23T11:25:50.196387Z"
    }
   },
   "outputs": [],
   "source": [
    "fullset = pd.read_csv('fullset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-23T06:07:35.653645Z",
     "start_time": "2019-07-23T06:07:35.646127Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3999995</th>\n",
       "      <td>0</td>\n",
       "      <td>Unbelievable- In a Bad Way We bought this Thom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999996</th>\n",
       "      <td>0</td>\n",
       "      <td>Almost Great, Until it Broke... My son recieve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999997</th>\n",
       "      <td>0</td>\n",
       "      <td>Disappointed !!! I bought this toy for my son ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999998</th>\n",
       "      <td>1</td>\n",
       "      <td>Classic Jessica Mitford This is a compilation ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999999</th>\n",
       "      <td>0</td>\n",
       "      <td>Comedy Scene, and Not Heard This DVD will be a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Sentiment                                               Text\n",
       "3999995          0  Unbelievable- In a Bad Way We bought this Thom...\n",
       "3999996          0  Almost Great, Until it Broke... My son recieve...\n",
       "3999997          0  Disappointed !!! I bought this toy for my son ...\n",
       "3999998          1  Classic Jessica Mitford This is a compilation ...\n",
       "3999999          0  Comedy Scene, and Not Heard This DVD will be a..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fullset.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-23T06:07:35.738285Z",
     "start_time": "2019-07-23T06:07:35.686252Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    2000000\n",
       "0    2000000\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fullset['Sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-23T11:26:09.818186Z",
     "start_time": "2019-07-23T11:26:08.475277Z"
    }
   },
   "outputs": [],
   "source": [
    "fullset['Text'] = fullset['Text'].apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-23T06:07:38.821560Z",
     "start_time": "2019-07-23T06:07:37.135285Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    4000000.0\n",
       "mean         431.0\n",
       "std          238.0\n",
       "min            3.0\n",
       "50%          382.0\n",
       "95%          894.0\n",
       "99%          988.0\n",
       "max         1014.0\n",
       "Name: Text, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(fullset['Text'].apply(len)).describe(percentiles=[.95, .99]).round(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text lowercase + Stemming\n",
    "def pre_proc(text):\n",
    "    return ''.join([PorterStemmer().stem(x) for x in Tokenizer.tokenize(text.lower()) if x != ''])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress bar: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 571428/571428 [53:35<00:00, 177.72it/s]  \n",
      "Progress bar: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 571428/571428 [53:53<00:00, 176.70it/s]\n",
      "Progress bar: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 571429/571429 [54:11<00:00, 175.73it/s]\n",
      "Progress bar: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 571428/571428 [54:45<00:00, 173.95it/s]\n",
      "Progress bar: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 571429/571429 [54:50<00:00, 173.65it/s]\n",
      "Progress bar: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 571429/571429 [55:10<00:00, 172.60it/s]\n",
      "Progress bar: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 571429/571429 [55:31<00:00, 171.52it/s]\n"
     ]
    }
   ],
   "source": [
    "fullset['Token'] = multi_apply(fullset['Text'], pre_proc, workers=cores-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-23T19:22:07.939387Z",
     "start_time": "2019-07-23T19:22:04.774297Z"
    }
   },
   "outputs": [],
   "source": [
    "fullset = pd.read_csv('fullset_ez_processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-23T06:22:25.408953Z",
     "start_time": "2019-07-23T06:22:25.405734Z"
    }
   },
   "outputs": [],
   "source": [
    "fdist = nltk.FreqDist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-23T06:27:44.091058Z",
     "start_time": "2019-07-23T06:22:26.076378Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in fullset['Token']:\n",
    "    for word in i.split():\n",
    "        fdist[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-23T06:38:18.409568Z",
     "start_time": "2019-07-23T06:38:15.842029Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     1804633.0\n",
       "mean          175.0\n",
       "std         21403.0\n",
       "min             1.0\n",
       "50%             1.0\n",
       "95%            17.0\n",
       "99%           271.0\n",
       "max      15792716.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count the number of UNIQUE word\n",
    "features = pd.Series(dict(fdist))\n",
    "features.describe(percentiles=[.95, .99]).round(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-23T06:45:33.109686Z",
     "start_time": "2019-07-23T06:45:33.051521Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1246917 words which only appear once.\n"
     ]
    }
   ],
   "source": [
    "features_1 = features[features==1]\n",
    "print('There are',len(features_1),'features which only appear once.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-23T07:24:25.352661Z",
     "start_time": "2019-07-23T07:24:25.348440Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some once features are like this: 'peaceful.on'\n",
      "So need to re-split them.\n"
     ]
    }
   ],
   "source": [
    "print('Some once features are like this:','\\''+features_1.index[1]+'\\'')\n",
    "print('So need to re-split them.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-23T09:45:43.765784Z",
     "start_time": "2019-07-23T09:45:43.679005Z"
    }
   },
   "outputs": [],
   "source": [
    "features_re = features[features<=1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-23T09:45:56.312619Z",
     "start_time": "2019-07-23T09:45:56.024765Z"
    }
   },
   "outputs": [],
   "source": [
    "relist = [x for x in features_1.index if (not x.isalpha())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-23T09:45:57.167370Z",
     "start_time": "2019-07-23T09:45:57.162816Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 824988 features with punctuations.\n"
     ]
    }
   ],
   "source": [
    "print('There are',len(relist),'features with punctuations.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-23T09:46:05.940756Z",
     "start_time": "2019-07-23T09:46:05.879325Z"
    }
   },
   "outputs": [],
   "source": [
    "relist_str = ''.join(relist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-23T09:55:36.503859Z",
     "start_time": "2019-07-23T09:55:35.579512Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üé•|Ã£|‚àÇ|üòÅ|‚Äû|'|ÔÄ†|7|6|Ãá|~|-|üò†|üòî|]|‚â†|‚òÜ|#|üòç|/|[|9|„Äê|.|_|üò©|‚Üì|5|‚Ç§|\u0007|)|‚äñ|üëè|‚ô†|8|üíú|1|‚àÖ|‚Ñâ|üò°|&|\u0015|‚êü|‚ïö|üëé|Ÿã|üéâ|$|üíÖ|Ôºà|üíñ|‚å´|:|üòÄ|}|,|‚ü®|\u0017|üòé|0|=|>|‚ïù|\"|‚òº|%|<|‚ô°|‚äï|;|„Äë|4|‚Ä≤|\u0002|+|?|‚Ä†|ÔÉî|^|‚äÇ|\\|Ôºâ|‚Äº|{|!|2|üòâ|ÃÑ|@|*|‚îÄ|\u0016|‚ô£|||(|3 \n",
      "\n",
      ":movie_camera:|Ã£|‚àÇ|:beaming_face_with_smiling_eyes:|‚Äû|'|ÔÄ†|7|6|Ãá|~|-|:angry_face:|:pensive_face:|]|‚â†|‚òÜ|#|:smiling_face_with_heart-eyes:|/|[|9|„Äê|.|_|:weary_face:|‚Üì|5|‚Ç§|\u0007|)|‚äñ|:clapping_hands:|:spade_suit:|8|:purple_heart:|1|‚àÖ|‚Ñâ|:pouting_face:|&|\u0015|‚êü|‚ïö|:thumbs_down:|Ÿã|:party_popper:|$|:nail_polish:|Ôºà|:sparkling_heart:|‚å´|:|:grinning_face:|}|,|‚ü®|\u0017|:smiling_face_with_sunglasses:|0|=|>|‚ïù|\"|‚òº|%|<|‚ô°|‚äï|;|„Äë|4|‚Ä≤|\u0002|+|?|‚Ä†|ÔÉî|^|‚äÇ|\\|Ôºâ|:double_exclamation_mark:|{|!|2|:winking_face:|ÃÑ|@|*|‚îÄ|\u0016|:club_suit:|||(|3\n"
     ]
    }
   ],
   "source": [
    "separator = '|'.join(list(set([r'{}'.format(x) for x in relist_str if not x.isalpha()])))\n",
    "print(separator,'\\n')\n",
    "print(demojize(separator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-23T19:22:37.242092Z",
     "start_time": "2019-07-23T19:22:37.238068Z"
    }
   },
   "outputs": [],
   "source": [
    "def resplit(text):\n",
    "    # Translate emojis\n",
    "    text = demojize(text)\n",
    "    # Remove punctuation\n",
    "    for i in string.punctuation:\n",
    "        text = text.replace(i,' ')\n",
    "    # Token\n",
    "    text = Tokenizer.tokenize(text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullset.Token = fullset.Token.apply(resplit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3999995</th>\n",
       "      <td>0</td>\n",
       "      <td>[unbeliev, in, a, bad, way, we, bought, thi, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999996</th>\n",
       "      <td>0</td>\n",
       "      <td>[almost, great, until, it, broke, my, son, rec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999997</th>\n",
       "      <td>0</td>\n",
       "      <td>[disappoint, i, bought, thi, toy, for, my, son...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999998</th>\n",
       "      <td>1</td>\n",
       "      <td>[classic, jessica, mitford, thi, is, a, compil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999999</th>\n",
       "      <td>0</td>\n",
       "      <td>[comedi, scene, and, not, heard, thi, dvd, wil...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Sentiment                                              Token\n",
       "3999995          0  [unbeliev, in, a, bad, way, we, bought, thi, t...\n",
       "3999996          0  [almost, great, until, it, broke, my, son, rec...\n",
       "3999997          0  [disappoint, i, bought, thi, toy, for, my, son...\n",
       "3999998          1  [classic, jessica, mitford, thi, is, a, compil...\n",
       "3999999          0  [comedi, scene, and, not, heard, thi, dvd, wil..."
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fullset.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('fullset_resplit.pickle', 'wb') as f:\n",
    "    pickle.dump(fullset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-07-23T12:11:34.298Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4000000/4000000 [06:07<00:00, 10893.40it/s]\n"
     ]
    }
   ],
   "source": [
    "# Re-count\n",
    "fdist = nltk.FreqDist()\n",
    "\n",
    "for i in tqdm(fullset['Token']):\n",
    "    for word in i:\n",
    "        fdist[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'the': 15809493, 'i': 9294446, 'and': 8586104, 'a': 8061406, 'to': 7712911, 'it': 7578653, 'of': 6298197, 'thi': 5911844, 'is': 5530069, 'in': 3713530, ...})"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-07-23T12:11:35.332Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count      858011.0\n",
       "mean          375.0\n",
       "std         31793.0\n",
       "min             1.0\n",
       "50%             1.0\n",
       "95%            51.0\n",
       "99%          1065.0\n",
       "max      15809493.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count the number of UNIQUE word\n",
    "features = pd.Series(dict(fdist))\n",
    "features.describe(percentiles=[.95, .99]).round(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-07-23T12:11:36.529Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 47820 features which only appear once.\n"
     ]
    }
   ],
   "source": [
    "features_1 = features[features==3]\n",
    "print('There are',len(features_1),'features which appear only once.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-07-23T12:11:37.276Z"
    }
   },
   "outputs": [],
   "source": [
    "def rmonce(token):\n",
    "    return [x for x in token if x not in features_1.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-07-23T12:11:37.903Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress bar: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4000000/4000000 [14:54<00:00, 4471.89it/s]   \n"
     ]
    }
   ],
   "source": [
    "# Remove words which appear only once.\n",
    "fullset.Token = fullset.Token.progress_apply(rmonce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    4000000.0\n",
       "mean          80.0\n",
       "std           44.0\n",
       "min            0.0\n",
       "50%           72.0\n",
       "95%          165.0\n",
       "99%          186.0\n",
       "max          257.0\n",
       "Name: Token, dtype: float64"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count the length of sentences\n",
    "(fullset.Token.apply(len)).describe(percentiles=[.95, .99]).round(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullset_original = pd.read_csv('fullset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "294435     ........ ............ ..... ..... ...... ........\n",
      "3584048    -_- ' ' '''' '''' '' '' ''' '''''? '' '' ' '' ...\n",
      "Name: Text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(fullset_original.Text[fullset.Token.apply(len)==0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullset = fullset[fullset.Token.apply(len)!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    3999998.0\n",
       "mean          80.0\n",
       "std           44.0\n",
       "min            1.0\n",
       "50%           72.0\n",
       "95%          165.0\n",
       "99%          186.0\n",
       "max          257.0\n",
       "Name: Token, dtype: float64"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(fullset.Token.apply(len)).describe(percentiles=[.95, .99]).round(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('fullset_resplit_rmonce.pickle', 'wb') as f:\n",
    "    pickle.dump(fullset, f, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 15809493),\n",
       " ('i', 9294446),\n",
       " ('and', 8586104),\n",
       " ('a', 8061406),\n",
       " ('to', 7712911),\n",
       " ('it', 7578653),\n",
       " ('of', 6298197),\n",
       " ('thi', 5911844),\n",
       " ('is', 5530069),\n",
       " ('in', 3713530),\n",
       " ('for', 3524304),\n",
       " ('that', 3245780),\n",
       " ('you', 2776538),\n",
       " ('wa', 2680975),\n",
       " ('not', 2612517),\n",
       " ('book', 2498410),\n",
       " ('but', 2345642),\n",
       " ('with', 2308551),\n",
       " ('on', 2281343),\n",
       " ('have', 2190331)]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The most frequent 20 words\n",
    "fdist.most_common()[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwordls = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmstopword(token):\n",
    "    return [x for x in token if x not in stopwordls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stopwords\n",
    "fullset.Token = fullset.Token.apply(rmstopword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-count\n",
    "fdist = nltk.FreqDist()\n",
    "\n",
    "for i in fullset['Token']:\n",
    "    for word in i:\n",
    "        fdist[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('thi', 5911844),\n",
       " ('wa', 2680975),\n",
       " ('book', 2498410),\n",
       " ('one', 1590682),\n",
       " ('like', 1289538),\n",
       " ('great', 1201557),\n",
       " ('veri', 1183127),\n",
       " ('good', 1167851),\n",
       " ('read', 1079779),\n",
       " ('use', 1002323),\n",
       " ('get', 995575),\n",
       " ('time', 944117),\n",
       " ('would', 939644),\n",
       " ('work', 875548),\n",
       " ('ha', 868094),\n",
       " ('movi', 773386),\n",
       " ('love', 772790),\n",
       " ('onli', 714689),\n",
       " ('hi', 665482),\n",
       " ('realli', 639812)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The most frequent 20 words\n",
    "fdist.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "810038"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many UNIQUE words\n",
    "len(fdist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    3999998.0\n",
       "mean          44.0\n",
       "std           24.0\n",
       "min            1.0\n",
       "50%           39.0\n",
       "95%           90.0\n",
       "99%          102.0\n",
       "max          212.0\n",
       "Name: Token, dtype: float64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(fullset.Token.apply(len)).describe(percentiles=[.95, .99]).round(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most frequent 55 words have 25% portion.\n",
      "The most frequent 294 words have 50% portion.\n",
      "The most frequent 1263 words have 75% portion.\n",
      "The most frequent 10323 words have 95% portion.\n",
      "The most frequent 68782 words have 99% portion.\n"
     ]
    }
   ],
   "source": [
    "fq25, fq50, fq75, fq95, fq99 = 0, 0, 0, 0, 0\n",
    "count = 0\n",
    "for i in fdist_df[0]:\n",
    "    fq25 += fdist.freq(i)\n",
    "    fq50 += fdist.freq(i)\n",
    "    fq75 += fdist.freq(i)\n",
    "    fq95 += fdist.freq(i)\n",
    "    fq99 += fdist.freq(i)\n",
    "    count += 1\n",
    "    if fq25 > 0.25:\n",
    "        print('The most frequent',count, 'words have 25% portion.')\n",
    "        fq25 -= 1\n",
    "    if fq50 > 0.50:\n",
    "        print('The most frequent',count, 'words have 50% portion.')\n",
    "        fq50 -= 1\n",
    "    if fq75 > 0.75:\n",
    "        print('The most frequent',count, 'words have 75% portion.')\n",
    "        fq75 -= 1\n",
    "    if fq95 > 0.95:\n",
    "        print('The most frequent',count, 'words have 95% portion.')\n",
    "        fq95 -= 1\n",
    "    if fq99 > 0.99:\n",
    "        print('The most frequent',count, 'words have 99% portion.')\n",
    "        fq99 -= 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('fullset_resplit_rmonce_nostopword.pickle', 'wb') as f:\n",
    "    pickle.dump(fullset, f, -1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
