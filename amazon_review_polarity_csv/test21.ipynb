{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-23T19:21:24.106764Z",
     "start_time": "2019-07-23T19:21:22.027760Z"
    }
   },
   "outputs": [],
   "source": [
    "# Packages\n",
    "from time import time\n",
    "import re\n",
    "import os\n",
    "import nltk\n",
    "import numpy as np\n",
    "import multiprocessing as mp\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "import multiprocessing as mp\n",
    "import pickle\n",
    "import string\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from emoji import demojize\n",
    "\n",
    "from sklearn import naive_bayes, svm, linear_model\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, roc_auc_score, f1_score, confusion_matrix, roc_curve\n",
    "from statlearning import plot_confusion_matrix\n",
    "\n",
    "# Settings\n",
    "sns.set_context('notebook') \n",
    "sns.set_style('ticks') \n",
    "colours = ['#1F77B4', '#FF7F0E', '#2CA02C', '#DB2728', '#9467BD', '#8C564B', '#E377C2','#7F7F7F', '#BCBD22', '#17BECF']\n",
    "crayon = ['#4E79A7','#F28E2C','#E15759','#76B7B2','#59A14F', '#EDC949','#AF7AA1','#FF9DA7','#9C755F','#BAB0AB']\n",
    "sns.set_palette(colours)\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (9, 6)\n",
    "tqdm.pandas(desc=\"Progress bar\")\n",
    "cores = mp.cpu_count()\n",
    "warnings.filterwarnings('ignore')\n",
    "Tokenizer = TweetTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-23T19:24:45.854986Z",
     "start_time": "2019-07-23T19:24:45.845935Z"
    }
   },
   "outputs": [],
   "source": [
    "def _apply_df(args):\n",
    "    df, func, kwargs = args\n",
    "    return df.progress_apply(func, **kwargs)\n",
    "\n",
    "def multi_apply(df, func, **kwargs):\n",
    "    workers = kwargs.pop('workers')\n",
    "    pool = mp.Pool(processes=workers)\n",
    "    result = pool.map(_apply_df, [(d, func, kwargs) for d in np.array_split(df, workers)])\n",
    "    pool.close()\n",
    "    return pd.concat(list(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv', header=None)\n",
    "\n",
    "# Merge title and content\n",
    "train['Text'] = train[1]+' '+train[2]\n",
    "train = train.drop(columns=[1,2])\n",
    "\n",
    "# Negative = 0, Positive = 1\n",
    "train[0] = train[0].map(lambda x: x-1)\n",
    "train.rename(columns={0:'Sentiment'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('test.csv', header=None)\n",
    "\n",
    "# Merge title and content\n",
    "test['Text'] = test[1]+' '+test[2]\n",
    "test = test.drop(columns=[1,2])\n",
    "\n",
    "# Negative = 0, Positive = 1\n",
    "test[0] = test[0].map(lambda x: x-1)\n",
    "test.rename(columns={0:'Sentiment'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge dataset\n",
    "fullset = pd.concat([train,test], axis=0, ignore_index=True)\n",
    "del(train)\n",
    "del(test)\n",
    "fullset.to_csv('fullset.csv', index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-23T11:26:08.439424Z",
     "start_time": "2019-07-23T11:25:50.196387Z"
    }
   },
   "outputs": [],
   "source": [
    "fullset = pd.read_csv('fullset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-23T06:07:35.653645Z",
     "start_time": "2019-07-23T06:07:35.646127Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3999995</th>\n",
       "      <td>0</td>\n",
       "      <td>Unbelievable- In a Bad Way We bought this Thom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999996</th>\n",
       "      <td>0</td>\n",
       "      <td>Almost Great, Until it Broke... My son recieve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999997</th>\n",
       "      <td>0</td>\n",
       "      <td>Disappointed !!! I bought this toy for my son ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999998</th>\n",
       "      <td>1</td>\n",
       "      <td>Classic Jessica Mitford This is a compilation ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999999</th>\n",
       "      <td>0</td>\n",
       "      <td>Comedy Scene, and Not Heard This DVD will be a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Sentiment                                               Text\n",
       "3999995          0  Unbelievable- In a Bad Way We bought this Thom...\n",
       "3999996          0  Almost Great, Until it Broke... My son recieve...\n",
       "3999997          0  Disappointed !!! I bought this toy for my son ...\n",
       "3999998          1  Classic Jessica Mitford This is a compilation ...\n",
       "3999999          0  Comedy Scene, and Not Heard This DVD will be a..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fullset.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-23T06:07:35.738285Z",
     "start_time": "2019-07-23T06:07:35.686252Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    2000000\n",
       "0    2000000\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fullset['Sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-23T11:26:09.818186Z",
     "start_time": "2019-07-23T11:26:08.475277Z"
    }
   },
   "outputs": [],
   "source": [
    "fullset['Text'] = fullset['Text'].apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-23T06:07:38.821560Z",
     "start_time": "2019-07-23T06:07:37.135285Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    4000000.0\n",
       "mean         431.0\n",
       "std          238.0\n",
       "min            3.0\n",
       "50%          382.0\n",
       "95%          894.0\n",
       "99%          988.0\n",
       "max         1014.0\n",
       "Name: Text, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(fullset['Text'].apply(len)).describe(percentiles=[.95, .99]).round(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text lowercase + Stemming\n",
    "def pre_proc(text):\n",
    "    return ''.join([PorterStemmer().stem(x) for x in Tokenizer.tokenize(text.lower()) if x != ''])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress bar: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 571428/571428 [53:35<00:00, 177.72it/s]  \n",
      "Progress bar: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 571428/571428 [53:53<00:00, 176.70it/s]\n",
      "Progress bar: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 571429/571429 [54:11<00:00, 175.73it/s]\n",
      "Progress bar: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 571428/571428 [54:45<00:00, 173.95it/s]\n",
      "Progress bar: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 571429/571429 [54:50<00:00, 173.65it/s]\n",
      "Progress bar: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 571429/571429 [55:10<00:00, 172.60it/s]\n",
      "Progress bar: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 571429/571429 [55:31<00:00, 171.52it/s]\n"
     ]
    }
   ],
   "source": [
    "fullset['Token'] = multi_apply(fullset['Text'], pre_proc, workers=cores-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-23T19:22:07.939387Z",
     "start_time": "2019-07-23T19:22:04.774297Z"
    }
   },
   "outputs": [],
   "source": [
    "fullset = pd.read_csv('fullset_ez_processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-23T06:22:25.408953Z",
     "start_time": "2019-07-23T06:22:25.405734Z"
    }
   },
   "outputs": [],
   "source": [
    "fdist = nltk.FreqDist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-23T06:27:44.091058Z",
     "start_time": "2019-07-23T06:22:26.076378Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in fullset['Token']:\n",
    "    for word in i.split():\n",
    "        fdist[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-23T06:38:18.409568Z",
     "start_time": "2019-07-23T06:38:15.842029Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     1804633.0\n",
       "mean          175.0\n",
       "std         21403.0\n",
       "min             1.0\n",
       "50%             1.0\n",
       "95%            17.0\n",
       "99%           271.0\n",
       "max      15792716.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count the number of UNIQUE word\n",
    "features = pd.Series(dict(fdist))\n",
    "features.describe(percentiles=[.95, .99]).round(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-23T06:45:33.109686Z",
     "start_time": "2019-07-23T06:45:33.051521Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1246917 words which only appear once.\n"
     ]
    }
   ],
   "source": [
    "features_1 = features[features==1]\n",
    "print('There are',len(features_1),'features which only appear once.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-23T07:24:25.352661Z",
     "start_time": "2019-07-23T07:24:25.348440Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some once features are like this: 'peaceful.on'\n",
      "So need to re-split them.\n"
     ]
    }
   ],
   "source": [
    "print('Some once features are like this:','\\''+features_1.index[1]+'\\'')\n",
    "print('So need to re-split them.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-23T09:45:43.765784Z",
     "start_time": "2019-07-23T09:45:43.679005Z"
    }
   },
   "outputs": [],
   "source": [
    "features_re = features[features<=1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-23T09:45:56.312619Z",
     "start_time": "2019-07-23T09:45:56.024765Z"
    }
   },
   "outputs": [],
   "source": [
    "relist = [x for x in features_1.index if (not x.isalpha())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-23T09:45:57.167370Z",
     "start_time": "2019-07-23T09:45:57.162816Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 824988 features with punctuations.\n"
     ]
    }
   ],
   "source": [
    "print('There are',len(relist),'features with punctuations.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-23T09:46:05.940756Z",
     "start_time": "2019-07-23T09:46:05.879325Z"
    }
   },
   "outputs": [],
   "source": [
    "relist_str = ''.join(relist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-23T09:55:36.503859Z",
     "start_time": "2019-07-23T09:55:35.579512Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üé•|Ã£|‚àÇ|üòÅ|‚Äû|'|ÔÄ†|7|6|Ãá|~|-|üò†|üòî|]|‚â†|‚òÜ|#|üòç|/|[|9|„Äê|.|_|üò©|‚Üì|5|‚Ç§|\u0007|)|‚äñ|üëè|‚ô†|8|üíú|1|‚àÖ|‚Ñâ|üò°|&|\u0015|‚êü|‚ïö|üëé|Ÿã|üéâ|$|üíÖ|Ôºà|üíñ|‚å´|:|üòÄ|}|,|‚ü®|\u0017|üòé|0|=|>|‚ïù|\"|‚òº|%|<|‚ô°|‚äï|;|„Äë|4|‚Ä≤|\u0002|+|?|‚Ä†|ÔÉî|^|‚äÇ|\\|Ôºâ|‚Äº|{|!|2|üòâ|ÃÑ|@|*|‚îÄ|\u0016|‚ô£|||(|3 \n",
      "\n",
      ":movie_camera:|Ã£|‚àÇ|:beaming_face_with_smiling_eyes:|‚Äû|'|ÔÄ†|7|6|Ãá|~|-|:angry_face:|:pensive_face:|]|‚â†|‚òÜ|#|:smiling_face_with_heart-eyes:|/|[|9|„Äê|.|_|:weary_face:|‚Üì|5|‚Ç§|\u0007|)|‚äñ|:clapping_hands:|:spade_suit:|8|:purple_heart:|1|‚àÖ|‚Ñâ|:pouting_face:|&|\u0015|‚êü|‚ïö|:thumbs_down:|Ÿã|:party_popper:|$|:nail_polish:|Ôºà|:sparkling_heart:|‚å´|:|:grinning_face:|}|,|‚ü®|\u0017|:smiling_face_with_sunglasses:|0|=|>|‚ïù|\"|‚òº|%|<|‚ô°|‚äï|;|„Äë|4|‚Ä≤|\u0002|+|?|‚Ä†|ÔÉî|^|‚äÇ|\\|Ôºâ|:double_exclamation_mark:|{|!|2|:winking_face:|ÃÑ|@|*|‚îÄ|\u0016|:club_suit:|||(|3\n"
     ]
    }
   ],
   "source": [
    "separator = '|'.join(list(set([r'{}'.format(x) for x in relist_str if not x.isalpha()])))\n",
    "print(separator,'\\n')\n",
    "print(demojize(separator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-23T19:22:37.242092Z",
     "start_time": "2019-07-23T19:22:37.238068Z"
    }
   },
   "outputs": [],
   "source": [
    "def resplit(text):\n",
    "    # Translate emojis\n",
    "    text = demojize(text)\n",
    "    # Remove punctuation\n",
    "    for i in string.punctuation:\n",
    "        text = text.replace(i,' ')\n",
    "    # Token\n",
    "    text = Tokenizer.tokenize(text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullset.Token = fullset.Token.apply(resplit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3999995</th>\n",
       "      <td>0</td>\n",
       "      <td>[unbeliev, in, a, bad, way, we, bought, thi, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999996</th>\n",
       "      <td>0</td>\n",
       "      <td>[almost, great, until, it, broke, my, son, rec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999997</th>\n",
       "      <td>0</td>\n",
       "      <td>[disappoint, i, bought, thi, toy, for, my, son...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999998</th>\n",
       "      <td>1</td>\n",
       "      <td>[classic, jessica, mitford, thi, is, a, compil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999999</th>\n",
       "      <td>0</td>\n",
       "      <td>[comedi, scene, and, not, heard, thi, dvd, wil...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Sentiment                                              Token\n",
       "3999995          0  [unbeliev, in, a, bad, way, we, bought, thi, t...\n",
       "3999996          0  [almost, great, until, it, broke, my, son, rec...\n",
       "3999997          0  [disappoint, i, bought, thi, toy, for, my, son...\n",
       "3999998          1  [classic, jessica, mitford, thi, is, a, compil...\n",
       "3999999          0  [comedi, scene, and, not, heard, thi, dvd, wil..."
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fullset.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('fullset_resplit.pickle', 'wb') as f:\n",
    "    pickle.dump(fullset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-07-23T12:11:34.298Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4000000/4000000 [06:07<00:00, 10893.40it/s]\n"
     ]
    }
   ],
   "source": [
    "# Re-count\n",
    "fdist = nltk.FreqDist()\n",
    "\n",
    "for i in tqdm(fullset['Token']):\n",
    "    for word in i:\n",
    "        fdist[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'the': 15809493, 'i': 9294446, 'and': 8586104, 'a': 8061406, 'to': 7712911, 'it': 7578653, 'of': 6298197, 'thi': 5911844, 'is': 5530069, 'in': 3713530, ...})"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-07-23T12:11:35.332Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count      858011.0\n",
       "mean          375.0\n",
       "std         31793.0\n",
       "min             1.0\n",
       "50%             1.0\n",
       "95%            51.0\n",
       "99%          1065.0\n",
       "max      15809493.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count the number of UNIQUE word\n",
    "features = pd.Series(dict(fdist))\n",
    "features.describe(percentiles=[.95, .99]).round(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-07-23T12:11:36.529Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 47820 features which only appear once.\n"
     ]
    }
   ],
   "source": [
    "features_1 = features[features==3]\n",
    "print('There are',len(features_1),'features which appear only once.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-07-23T12:11:37.276Z"
    }
   },
   "outputs": [],
   "source": [
    "def rmonce(token):\n",
    "    return [x for x in token if x not in features_1.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-07-23T12:11:37.903Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress bar: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4000000/4000000 [14:54<00:00, 4471.89it/s]   \n"
     ]
    }
   ],
   "source": [
    "# Remove words which appear only once.\n",
    "fullset.Token = fullset.Token.progress_apply(rmonce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    4000000.0\n",
       "mean          80.0\n",
       "std           44.0\n",
       "min            0.0\n",
       "50%           72.0\n",
       "95%          165.0\n",
       "99%          186.0\n",
       "max          257.0\n",
       "Name: Token, dtype: float64"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count the length of sentences\n",
    "(fullset.Token.apply(len)).describe(percentiles=[.95, .99]).round(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullset_original = pd.read_csv('fullset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "294435     ........ ............ ..... ..... ...... ........\n",
      "3584048    -_- ' ' '''' '''' '' '' ''' '''''? '' '' ' '' ...\n",
      "Name: Text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(fullset_original.Text[fullset.Token.apply(len)==0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullset = fullset[fullset.Token.apply(len)!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    3999998.0\n",
       "mean          80.0\n",
       "std           44.0\n",
       "min            1.0\n",
       "50%           72.0\n",
       "95%          165.0\n",
       "99%          186.0\n",
       "max          257.0\n",
       "Name: Token, dtype: float64"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(fullset.Token.apply(len)).describe(percentiles=[.95, .99]).round(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('fullset_resplit_rmonce.pickle', 'wb') as f:\n",
    "    pickle.dump(fullset, f, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 15809493),\n",
       " ('i', 9294446),\n",
       " ('and', 8586104),\n",
       " ('a', 8061406),\n",
       " ('to', 7712911),\n",
       " ('it', 7578653),\n",
       " ('of', 6298197),\n",
       " ('thi', 5911844),\n",
       " ('is', 5530069),\n",
       " ('in', 3713530),\n",
       " ('for', 3524304),\n",
       " ('that', 3245780),\n",
       " ('you', 2776538),\n",
       " ('wa', 2680975),\n",
       " ('not', 2612517),\n",
       " ('book', 2498410),\n",
       " ('but', 2345642),\n",
       " ('with', 2308551),\n",
       " ('on', 2281343),\n",
       " ('have', 2190331)]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The most frequent 20 words\n",
    "fdist.most_common()[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwordls = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmstopword(token):\n",
    "    return [x for x in token if x not in stopwordls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stopwords\n",
    "fullset.Token = fullset.Token.apply(rmstopword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-count\n",
    "fdist = nltk.FreqDist()\n",
    "\n",
    "for i in fullset['Token']:\n",
    "    for word in i:\n",
    "        fdist[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('thi', 5911844),\n",
       " ('wa', 2680975),\n",
       " ('book', 2498410),\n",
       " ('one', 1590682),\n",
       " ('like', 1289538),\n",
       " ('great', 1201557),\n",
       " ('veri', 1183127),\n",
       " ('good', 1167851),\n",
       " ('read', 1079779),\n",
       " ('use', 1002323),\n",
       " ('get', 995575),\n",
       " ('time', 944117),\n",
       " ('would', 939644),\n",
       " ('work', 875548),\n",
       " ('ha', 868094),\n",
       " ('movi', 773386),\n",
       " ('love', 772790),\n",
       " ('onli', 714689),\n",
       " ('hi', 665482),\n",
       " ('realli', 639812)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The most frequent 20 words\n",
    "fdist.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "810038"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many UNIQUE words\n",
    "len(fdist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    3999998.0\n",
       "mean          44.0\n",
       "std           24.0\n",
       "min            1.0\n",
       "50%           39.0\n",
       "95%           90.0\n",
       "99%          102.0\n",
       "max          212.0\n",
       "Name: Token, dtype: float64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(fullset.Token.apply(len)).describe(percentiles=[.95, .99]).round(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most frequent 55 words have 25% portion.\n",
      "The most frequent 294 words have 50% portion.\n",
      "The most frequent 1263 words have 75% portion.\n",
      "The most frequent 10323 words have 95% portion.\n",
      "The most frequent 68782 words have 99% portion.\n"
     ]
    }
   ],
   "source": [
    "fq25, fq50, fq75, fq95, fq99 = 0, 0, 0, 0, 0\n",
    "count = 0\n",
    "for i in fdist_df[0]:\n",
    "    fq25 += fdist.freq(i)\n",
    "    fq50 += fdist.freq(i)\n",
    "    fq75 += fdist.freq(i)\n",
    "    fq95 += fdist.freq(i)\n",
    "    fq99 += fdist.freq(i)\n",
    "    count += 1\n",
    "    if fq25 > 0.25:\n",
    "        print('The most frequent',count, 'words have 25% portion.')\n",
    "        fq25 -= 1\n",
    "    if fq50 > 0.50:\n",
    "        print('The most frequent',count, 'words have 50% portion.')\n",
    "        fq50 -= 1\n",
    "    if fq75 > 0.75:\n",
    "        print('The most frequent',count, 'words have 75% portion.')\n",
    "        fq75 -= 1\n",
    "    if fq95 > 0.95:\n",
    "        print('The most frequent',count, 'words have 95% portion.')\n",
    "        fq95 -= 1\n",
    "    if fq99 > 0.99:\n",
    "        print('The most frequent',count, 'words have 99% portion.')\n",
    "        fq99 -= 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('fullset_resplit_rmonce_nostopword.pickle', 'wb') as f:\n",
    "    pickle.dump(fullset, f, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas.core.internals.managers'; 'pandas.core.internals' is not a package",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-00da286292b0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'fullset_resplit_rmonce_nostopword.pickle'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mfullset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas.core.internals.managers'; 'pandas.core.internals' is not a package"
     ]
    }
   ],
   "source": [
    "with open('fullset_resplit_rmonce_nostopword.pickle', 'rb') as f:\n",
    "    fullset = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, test_x, train_y, test_y = train_test_split(fullset['Token'], fullset['Sentiment'], train_size=0.8, random_state=1, stratify=fullset.Sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import DictVectorizer, CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "one_hot = DictVectorizer()\n",
    "one_hot.fit((fullset['Token']))\n",
    "print('# One-hot Fit:', time()-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "one_hot_bs = one_hot.transform(train_x)\n",
    "one_hot_bs = one_hot.transform(test_x)\n",
    "print('# One-hot Transform:', time()-t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count Vectors + N-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "count_vect_1n = CountVectorizer(analyzer='word', ngram_range=(1,1), token_pattern=r'\\w{1,}', max_features=10323)\n",
    "count_vect_1n.fit(fullset['Token'])\n",
    "print('# CV + 1-gram:', time()-t0)\n",
    "\n",
    "t0 = time()\n",
    "count_vect_2n = CountVectorizer(analyzer='word', ngram_range=(1,2), token_pattern=r'\\w{1,}', max_features=10323)\n",
    "count_vect_2n.fit(fullset['Token'])\n",
    "print('# CV + 2-gram:', time()-t0)\n",
    "\n",
    "t0 = time()\n",
    "count_vect_3n = CountVectorizer(analyzer='word', ngram_range=(1,3), token_pattern=r'\\w{1,}', max_features=10323)\n",
    "count_vect_3n.fit(fullset['Token'])\n",
    "print('# CV + 3-gram:', time()-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the training and testing data using count vectorizer object\n",
    "t0 = time()\n",
    "xtrain_count_1n =  count_vect_1n.transform(train_x)\n",
    "xvalid_count_1n =  count_vect_1n.transform(test_x)\n",
    "print('# Finish CV 1n:', time()-t0)\n",
    "\n",
    "t0 = time()\n",
    "xtrain_count_2n =  count_vect_2n.transform(train_x)\n",
    "xvalid_count_2n =  count_vect_2n.transform(test_x)\n",
    "print('# Finish CV 2n:', time()-t0)\n",
    "\n",
    "t0 = time()\n",
    "xtrain_count_3n =  count_vect_3n.transform(train_x)\n",
    "xvalid_count_3n =  count_vect_3n.transform(test_x)\n",
    "print('# Finish CV 3n:', time()-t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Vectors + N-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# TF-IDF + 1-gram\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'TfidfVectorizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-6b518e22f7ef>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'# TF-IDF + 1-gram'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtfidf_vect_1n\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manalyzer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'word'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mngram_range\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtoken_pattern\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mr'\\w{1,}'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10323\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mtfidf_vect_1n\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfullset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Token'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'# TF-IDF + 1-gram:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mt0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'TfidfVectorizer' is not defined"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "tfidf_vect_1n = TfidfVectorizer(analyzer='word', ngram_range=(1,1), token_pattern=r'\\w{1,}', max_features=10323)\n",
    "tfidf_vect_1n.fit(fullset['Token'])\n",
    "print('# TF-IDF + 1-gram:', time()-t0)\n",
    "\n",
    "t0 = time()\n",
    "tfidf_vect_2n = TfidfVectorizer(analyzer='word', ngram_range=(1,2), token_pattern=r'\\w{1,}', max_features=10323)\n",
    "tfidf_vect_2n.fit(fullset['Token'])\n",
    "print('# TF-IDF + 2-gram:', time()-t0)\n",
    "\n",
    "t0 = time()\n",
    "tfidf_vect_3n = TfidfVectorizer(analyzer='word', ngram_range=(1,3), token_pattern=r'\\w{1,}', max_features=10323)\n",
    "tfidf_vect_3n.fit(fullset['Token'])\n",
    "print('# TF-IDF + 3-gram:', time()-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "xtrain_tfidf_1n =  tfidf_vect_1n.transform(train_x)\n",
    "xvalid_tfidf_1n =  tfidf_vect_1n.transform(test_x)\n",
    "print('# Finish TF-IDF 1n:', time()-t0)\n",
    "\n",
    "t0 = time()\n",
    "xtrain_tfidf_2n =  tfidf_vect_2n.transform(train_x)\n",
    "xvalid_tfidf_2n =  tfidf_vect_2n.transform(test_x)\n",
    "print('# Finish TF-IDF 2n:', time()-t0)\n",
    "\n",
    "t0 = time()\n",
    "xtrain_tfidf_3n =  tfidf_vect_3n.transform(train_x)\n",
    "xvalid_tfidf_3n =  tfidf_vect_3n.transform(test_x)\n",
    "print('# Finish TF-IDF 3n:', time()-t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traditional ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(classifier, feature_vector_train, label, feature_vector_valid, is_not_svm=True):\n",
    "    # fit the training dataset on the classifier\n",
    "    trm = classifier.fit(feature_vector_train, label)\n",
    "    \n",
    "    # predict the labels on validation dataset\n",
    "    predictions = trm.predict(feature_vector_valid)\n",
    "    \n",
    "    print('Accuracy:', accuracy_score(predictions, valid_y).round(4))\n",
    "    print('Precision:', precision_score(predictions, valid_y).round(4))\n",
    "    print('Recall:', recall_score(predictions, valid_y).round(4))\n",
    "    print('F1 Score:', f1_score(predictions, valid_y).round(4))\n",
    "    '''\n",
    "    confusion = confusion_matrix(predictions, valid_y)\n",
    "    fig, ax = plt.subplots(figsize=(8,6))\n",
    "    plot_confusion_matrix(confusion, classes=['negative','positive'], normalize=True)\n",
    "    plt.show()\n",
    "    '''\n",
    "    #print('AUC:', roc_auc_score(valid_y, prob[:,1]).round(4),'\\n')\n",
    "    if is_not_svm:\n",
    "        prob = trm.predict_proba(feature_vector_valid)\n",
    "        fpr, tpr, _ = roc_curve(valid_y, prob[:,1])\n",
    "        auc = roc_auc_score(valid_y, prob[:,1])\n",
    "\n",
    "        fig, ax= plt.subplots(figsize=(8,6))\n",
    "        ax.plot(fpr, tpr, label='ROC curve (AUC = {:.4f})'.format(auc))\n",
    "        ax.set_xlabel('False positive rate')\n",
    "        ax.set_ylabel('True Positive Rate')\n",
    "        ax.set_title('ROC curve', fontsize=14)\n",
    "        sns.despine()\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "train_model(naive_bayes.BernoulliNB(), xtrain_count_1n, train_y, xvalid_count_1n)\n",
    "print(\"# Naive Bayes + Count Vectors + 1-gram:\", time()-t0)\n",
    "\n",
    "t0 = time()\n",
    "train_model(naive_bayes.BernoulliNB(), xtrain_count_2n, train_y, xvalid_count_2n)\n",
    "print(\"# Naive Bayes + Count Vectors + 2-gram\", time()-t0)\n",
    "\n",
    "t0 = time()\n",
    "train_model(naive_bayes.BernoulliNB(), xtrain_count_3n, train_y, xvalid_count_3n)\n",
    "print(\"# Naive Bayes + Count Vectors + 3-gram\", time()-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "train_model(naive_bayes.BernoulliNB(), xtrain_tfidf_1n, train_y, xvalid_tfidf_1n)\n",
    "print(\"# Naive Bayes + TF-IDF + 1-gram\", time()-t0)\n",
    "\n",
    "t0 = time()\n",
    "train_model(naive_bayes.BernoulliNB(), xtrain_tfidf_2n, train_y, xvalid_tfidf_2n)\n",
    "print(\"# Naive Bayes + TF-IDF + 2-gram\", time()-t0)\n",
    "\n",
    "t0 = time()\n",
    "train_model(naive_bayes.BernoulliNB(), xtrain_tfidf_3n, train_y, xvalid_tfidf_3n)\n",
    "print(\"# Naive Bayes + TF-IDF + 3-gram\", time()-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "train_model(naive_bayes.BernoulliNB(), train_vec, train_y, valid_vec)\n",
    "print(\"# Naive Bayes + 1234 + Word2vec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "train_model(svm.LinearSVC(), xtrain_count_1n, train_y, xvalid_count_1n)\n",
    "print(\"# SVM + Count Vectors + 1-gram\", time()-t0)\n",
    "\n",
    "t0 = time()\n",
    "train_model(svm.LinearSVC(), xtrain_count_2n, train_y, xvalid_count_2n)\n",
    "print(\"# SVM + Count Vectors + 2-gram\", time()-t0)\n",
    "\n",
    "t0 = time()\n",
    "train_model(svm.LinearSVC(), xtrain_count_3n, train_y, xvalid_count_3n)\n",
    "print(\"# SVM + Count Vectors + 3-gram\", time()-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "train_model(svm.LinearSVC(), xtrain_tfidf_1n, train_y, xvalid_tfidf_1n)\n",
    "print(\"# SVM + TF-IDF + 1-gram\", time()-t0)\n",
    "\n",
    "t0 = time()\n",
    "train_model(svm.LinearSVC(), xtrain_tfidf_2n, train_y, xvalid_tfidf_2n)\n",
    "print(\"# SVM + TF-IDF + 2-gram\", time()-t0)\n",
    "\n",
    "t0 = time()\n",
    "train_model(svm.LinearSVC(), xtrain_tfidf_3n, train_y, xvalid_tfidf_3n)\n",
    "print(\"# SVM + TF-IDF + 3-gram\", time()-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "train_model(svm.LinearSVC(), train_vec, train_y, valid_vec)\n",
    "print(\"# SVM + Word2vec\", time()-t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "train_model(linear_model.LogisticRegression(), xtrain_count_1n, train_y, xvalid_count_1n)\n",
    "print(\"Logistic Regression + Count Vectors + 1-gram\", time()-t0)\n",
    "\n",
    "t0 = time()\n",
    "train_model(linear_model.LogisticRegression(), xtrain_count_2n, train_y, xvalid_count_2n)\n",
    "print(\"\\nLogistic Regression + Count Vectors + 2-gram\", time()-t0)\n",
    "\n",
    "t0 = time()\n",
    "train_model(linear_model.LogisticRegression(), xtrain_count_3n, train_y, xvalid_count_3n)\n",
    "print(\"\\nLogistic Regression + Count Vectors + 3-gram\", time()-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "train_model(linear_model.LogisticRegression(), xtrain_tfidf_1n, train_y, xvalid_tfidf_1n)\n",
    "print(\"Logistic Regression + TF-IDF + 1-gram\", time()-t0)\n",
    "\n",
    "t0 = time()\n",
    "train_model(linear_model.LogisticRegression(), xtrain_tfidf_2n, train_y, xvalid_tfidf_2n)\n",
    "print(\"\\nLogistic Regression + TF-IDF + 2-gram\", time()-t0)\n",
    "\n",
    "t0 = time()\n",
    "train_model(linear_model.LogisticRegression(), xtrain_tfidf_3n, train_y, xvalid_tfidf_3n)\n",
    "print(\"\\nLogistic Regression + TF-IDF + 3-gram\", time()-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "train_model(linear_model.LogisticRegression(), train_vec, train_y, valid_vec)\n",
    "print(\"# Logistic Regression + Word2vec\", time()-t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import text, sequence\n",
    "from keras import layers, models, optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM (RNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN + LSTM"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
