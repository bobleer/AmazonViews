{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T21:04:11.253370Z",
     "start_time": "2019-07-17T21:04:08.905695Z"
    }
   },
   "outputs": [],
   "source": [
    "# Packages\n",
    "import nltk\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T21:04:11.277228Z",
     "start_time": "2019-07-17T21:04:11.266827Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot settings\n",
    "sns.set_context('notebook') \n",
    "sns.set_style('ticks') \n",
    "colours = ['#1F77B4', '#FF7F0E', '#2CA02C', '#DB2728', '#9467BD', '#8C564B', '#E377C2','#7F7F7F', '#BCBD22', '#17BECF']\n",
    "crayon = ['#4E79A7','#F28E2C','#E15759','#76B7B2','#59A14F', '#EDC949','#AF7AA1','#FF9DA7','#9C755F','#BAB0AB']\n",
    "sns.set_palette(colours)\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (9, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T21:04:12.840566Z",
     "start_time": "2019-07-17T21:04:12.837665Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T21:04:38.514893Z",
     "start_time": "2019-07-17T21:04:13.909177Z"
    }
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv', header=None)\n",
    "train['Text'] = train[1]+' '+train[2]\n",
    "train = train.drop(columns=[1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T21:04:40.834379Z",
     "start_time": "2019-07-17T21:04:39.566952Z"
    }
   },
   "outputs": [],
   "source": [
    "train[0] = train[0].map(lambda x: x-1)\n",
    "train.rename(columns={0:'Sentiment'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T21:04:43.612218Z",
     "start_time": "2019-07-17T21:04:41.922776Z"
    }
   },
   "outputs": [],
   "source": [
    "# Generate a random and balanced small sample for coding \n",
    "train_pos = train[train['Sentiment']==1].sample(frac=1)[:2500]\n",
    "train_neg = train[train['Sentiment']==0].sample(frac=1)[:2500]\n",
    "\n",
    "train_sp = pd.concat([train_pos, train_neg], axis=0, ignore_index=True)\n",
    "train_sp = train_sp.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T21:04:44.860061Z",
     "start_time": "2019-07-17T21:04:44.845101Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>1</td>\n",
       "      <td>What version of Gobe Productive BeOS operating...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>0</td>\n",
       "      <td>AWFUL BELLY DANCING! Amira Mor is a disgrace t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>1</td>\n",
       "      <td>Now this is a classic screw all the family-ori...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>0</td>\n",
       "      <td>Hunk-O-Junk I agree with those who've said the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>1</td>\n",
       "      <td>Excellent!!! Tiffen 67mm Hollywood FX Glamour ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Sentiment                                               Text\n",
       "4995          1  What version of Gobe Productive BeOS operating...\n",
       "4996          0  AWFUL BELLY DANCING! Amira Mor is a disgrace t...\n",
       "4997          1  Now this is a classic screw all the family-ori...\n",
       "4998          0  Hunk-O-Junk I agree with those who've said the...\n",
       "4999          1  Excellent!!! Tiffen 67mm Hollywood FX Glamour ..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sp.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T21:04:49.383008Z",
     "start_time": "2019-07-17T21:04:49.369991Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    2500\n",
       "0    2500\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sp['Sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T21:04:51.640879Z",
     "start_time": "2019-07-17T21:04:51.637489Z"
    }
   },
   "outputs": [],
   "source": [
    "# 1. Tokenisation (casual module)\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "Tokenizer = TweetTokenizer()\n",
    "# 2. Remove punctuation\n",
    "import string\n",
    "# 3. Remove stopwords\n",
    "from nltk.corpus import stopwords\n",
    "# 4. Stemming\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T21:04:52.746303Z",
     "start_time": "2019-07-17T21:04:52.740871Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_text(text):\n",
    "    text = str(text)\n",
    "    tokenized = Tokenizer.tokenize(text)\n",
    "    tokenized_no_punctuation = [word.lower() for word in tokenized if word not in string.punctuation]\n",
    "    tokenized_no_stopwords = [word for word in tokenized_no_punctuation if word not in stopwords.words('english')]\n",
    "    token = [PorterStemmer().stem(word) for word in tokenized_no_stopwords if word != 'Ô∏è']\n",
    "    token = ' '.join(token)\n",
    "    return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T22:35:07.205015Z",
     "start_time": "2019-07-17T22:35:07.200380Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_text_ez(text):\n",
    "    text = str(text)\n",
    "    tokenized = Tokenizer.tokenize(text)\n",
    "    tokenized_lower = [word.lower() for word in tokenized]\n",
    "    tokenized_stem = [PorterStemmer().stem(word) for word in tokenized_lower if word != '']\n",
    "    token = ' '.join(tokenized_stem)\n",
    "    return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T21:06:02.101801Z",
     "start_time": "2019-07-17T21:04:53.958266Z"
    }
   },
   "outputs": [],
   "source": [
    "train_sp['Token'] = train_sp['Text'].apply(process_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T22:35:19.991900Z",
     "start_time": "2019-07-17T22:35:08.739528Z"
    }
   },
   "outputs": [],
   "source": [
    "train_sp['Text'] = train_sp['Text'].apply(process_text_ez)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T22:35:26.972765Z",
     "start_time": "2019-07-17T22:35:26.963862Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Text</th>\n",
       "      <th>Token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>1</td>\n",
       "      <td>what version of gobe product beo oper system i...</td>\n",
       "      <td>version gobe product beo oper system great fas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>0</td>\n",
       "      <td>aw belli danc ! amira mor is a disgrac to the ...</td>\n",
       "      <td>aw belli danc amira mor disgrac art belli danc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>1</td>\n",
       "      <td>now thi is a classic screw all the family-orie...</td>\n",
       "      <td>classic screw family-orient garbag taint movi ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>0</td>\n",
       "      <td>hunk-o-junk i agre with those who'v said the b...</td>\n",
       "      <td>hunk-o-junk agre who'v said batteri life phone...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>1</td>\n",
       "      <td>excel ! ! ! tiffen 67mm hollywood fx glamour f...</td>\n",
       "      <td>excel tiffen 67mm hollywood fx glamour filter ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Sentiment                                               Text  \\\n",
       "4995          1  what version of gobe product beo oper system i...   \n",
       "4996          0  aw belli danc ! amira mor is a disgrac to the ...   \n",
       "4997          1  now thi is a classic screw all the family-orie...   \n",
       "4998          0  hunk-o-junk i agre with those who'v said the b...   \n",
       "4999          1  excel ! ! ! tiffen 67mm hollywood fx glamour f...   \n",
       "\n",
       "                                                  Token  \n",
       "4995  version gobe product beo oper system great fas...  \n",
       "4996  aw belli danc amira mor disgrac art belli danc...  \n",
       "4997  classic screw family-orient garbag taint movi ...  \n",
       "4998  hunk-o-junk agre who'v said batteri life phone...  \n",
       "4999  excel tiffen 67mm hollywood fx glamour filter ...  "
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sp.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T22:35:38.614201Z",
     "start_time": "2019-07-17T22:35:38.605496Z"
    }
   },
   "outputs": [],
   "source": [
    "# Randomly split indexes 80/20\n",
    "train_x, valid_x, train_y, valid_y = train_test_split(train_sp['Token'], train_sp['Sentiment'], train_size=0.8, random_state=1, stratify=train_sp.Sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count Vectors (BoW) as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T22:35:44.866361Z",
     "start_time": "2019-07-17T22:35:42.826403Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=5000, min_df=1,\n",
       "        ngram_range=(3, 3), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='\\\\w{1,}', tokenizer=None,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# 1-gram\n",
    "count_vect_1n = CountVectorizer(analyzer='word', ngram_range=(1,1), token_pattern=r'\\w{1,}', max_features=5000)\n",
    "count_vect_1n.fit(train_sp['Token'])\n",
    "\n",
    "# 2-gram\n",
    "count_vect_2n = CountVectorizer(analyzer='word', ngram_range=(2,2), token_pattern=r'\\w{1,}', max_features=5000)\n",
    "count_vect_2n.fit(train_sp['Token'])\n",
    "\n",
    "# 3-gram\n",
    "count_vect_3n = CountVectorizer(analyzer='word', ngram_range=(3,3), token_pattern=r'\\w{1,}', max_features=5000)\n",
    "count_vect_3n.fit(train_sp['Token'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T22:35:45.813359Z",
     "start_time": "2019-07-17T22:35:45.087337Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transform the training and validation data using count vectorizer object\n",
    "xtrain_count_1n =  count_vect_1n.transform(train_x)\n",
    "xvalid_count_1n =  count_vect_1n.transform(valid_x)\n",
    "\n",
    "xtrain_count_2n =  count_vect_2n.transform(train_x)\n",
    "xvalid_count_2n =  count_vect_2n.transform(valid_x)\n",
    "\n",
    "xtrain_count_3n =  count_vect_3n.transform(train_x)\n",
    "xvalid_count_3n =  count_vect_3n.transform(valid_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Vectors + N-gram as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T22:35:51.764480Z",
     "start_time": "2019-07-17T22:35:49.683207Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=5000, min_df=1,\n",
       "        ngram_range=(3, 3), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='\\\\w{1,}', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1-gram\n",
    "tfidf_vect_1n = TfidfVectorizer(analyzer='word', ngram_range=(1,1), token_pattern=r'\\w{1,}', max_features=5000)\n",
    "tfidf_vect_1n.fit(train_sp['Token'])\n",
    "\n",
    "# 2-gram\n",
    "tfidf_vect_2n = TfidfVectorizer(analyzer='word', ngram_range=(2,2), token_pattern=r'\\w{1,}', max_features=5000)\n",
    "tfidf_vect_2n.fit(train_sp['Token'])\n",
    "\n",
    "# 3-gram\n",
    "tfidf_vect_3n = TfidfVectorizer(analyzer='word', ngram_range=(3,3), token_pattern=r'\\w{1,}', max_features=5000)\n",
    "tfidf_vect_3n.fit(train_sp['Token'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T22:35:52.807316Z",
     "start_time": "2019-07-17T22:35:52.009429Z"
    }
   },
   "outputs": [],
   "source": [
    "xtrain_tfidf_1n =  tfidf_vect_1n.transform(train_x)\n",
    "xvalid_tfidf_1n =  tfidf_vect_1n.transform(valid_x)\n",
    "\n",
    "xtrain_tfidf_2n =  tfidf_vect_2n.transform(train_x)\n",
    "xvalid_tfidf_2n =  tfidf_vect_2n.transform(valid_x)\n",
    "\n",
    "xtrain_tfidf_3n =  tfidf_vect_3n.transform(train_x)\n",
    "xvalid_tfidf_3n =  tfidf_vect_3n.transform(valid_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2vec as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T22:36:03.806074Z",
     "start_time": "2019-07-17T22:36:03.803056Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T22:36:06.670971Z",
     "start_time": "2019-07-17T22:36:04.427400Z"
    }
   },
   "outputs": [],
   "source": [
    "sentence = [(i.split()) for i in train_sp.Text]\n",
    "w2vmodel = word2vec.Word2Vec(sentence, min_count=1, size=300, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T22:36:08.320575Z",
     "start_time": "2019-07-17T22:36:08.270046Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('review', 0.9352136850357056),\n",
       " ('movi', 0.9123331308364868),\n",
       " ('one', 0.911530613899231),\n",
       " ('cd', 0.9013593792915344),\n",
       " ('dvd', 0.8915457725524902),\n",
       " ('album', 0.8906834721565247),\n",
       " ('teleplay', 0.8861536979675293),\n",
       " ('film', 0.8826782703399658),\n",
       " ('game', 0.8803355693817139),\n",
       " ('caliber.th', 0.8781176805496216)]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2vmodel.most_similar(['book'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T22:38:03.257469Z",
     "start_time": "2019-07-17T22:38:03.248880Z"
    }
   },
   "outputs": [],
   "source": [
    "train_w2v_x, valid_w2v_x, train_w2v_y, valid_w2v_y = train_test_split(train_sp['Text'], train_sp['Sentiment'], train_size=0.8, random_state=1, stratify=train_sp.Sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T22:39:00.719744Z",
     "start_time": "2019-07-17T22:39:00.713519Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_sent_vec(size, sent, model):\n",
    "    vec = np.zeros(size).reshape(1,size)\n",
    "    count = 0\n",
    "    for word in sent:\n",
    "        try:\n",
    "            vec += model[word].reshape(1,size)\n",
    "            count += 1\n",
    "        except:\n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec\n",
    "\n",
    "def get_train_vec(train_x, valid_x, model):\n",
    "    train_vec = np.concatenate([get_sent_vec(300, sent, model) for sent in train_x])\n",
    "    test_vec = np.concatenate([get_sent_vec(300, sent, model) for sent in valid_x])\n",
    "    return train_vec, test_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T22:44:26.914225Z",
     "start_time": "2019-07-17T22:43:51.585841Z"
    }
   },
   "outputs": [],
   "source": [
    "# Lower + No punctuation + No stopwords+ Stemming\n",
    "train_vec, valid_vec = get_train_vec(train_x, valid_x, w2vmodel)\n",
    "# Lower words + Stemming\n",
    "train_w2v_vec, valid_w2v_vec = get_train_vec(train_w2v_x, valid_w2v_x, w2vmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training (Traditional ML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T22:40:06.469689Z",
     "start_time": "2019-07-17T22:40:06.466371Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, roc_auc_score, f1_score, confusion_matrix, roc_curve\n",
    "from statlearning import plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T22:40:07.643944Z",
     "start_time": "2019-07-17T22:40:07.637427Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import naive_bayes, svm, linear_model\n",
    "\n",
    "def train_model(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net=False):\n",
    "    # fit the training dataset on the classifier\n",
    "    trm = classifier.fit(feature_vector_train, label)\n",
    "    \n",
    "    # predict the labels on validation dataset\n",
    "    predictions = trm.predict(feature_vector_valid)\n",
    "    \n",
    "    if is_neural_net:\n",
    "        predictions = predictions.argmax(axis=-1)\n",
    "\n",
    "    print('Accuracy:', accuracy_score(predictions, valid_y).round(4))\n",
    "    print('Precision:', precision_score(predictions, valid_y).round(4))\n",
    "    print('Recall:', recall_score(predictions, valid_y).round(4))\n",
    "    print('F1 Score:', f1_score(predictions, valid_y).round(4))\n",
    "    '''\n",
    "    confusion = confusion_matrix(predictions, valid_y)\n",
    "    fig, ax = plt.subplots(figsize=(8,6))\n",
    "    plot_confusion_matrix(confusion, classes=['negative','positive'], normalize=True)\n",
    "    plt.show()\n",
    "    '''\n",
    "    #print('AUC:', roc_auc_score(predictions, proba_y[:,1]).round(3),'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T22:40:11.128353Z",
     "start_time": "2019-07-17T22:40:11.096480Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Naive Bayes + Count Vectors + 1-gram\n",
      "Accuracy: 0.836\n",
      "Precision: 0.87\n",
      "Recall: 0.8146\n",
      "F1 Score: 0.8414\n",
      "\n",
      "# Naive Bayes + Count Vectors + 2-gram\n",
      "Accuracy: 0.753\n",
      "Precision: 0.82\n",
      "Recall: 0.7231\n",
      "F1 Score: 0.7685\n",
      "\n",
      "# Naive Bayes + Count Vectors + 3-gram\n",
      "Accuracy: 0.609\n",
      "Precision: 0.85\n",
      "Recall: 0.5735\n",
      "F1 Score: 0.6849\n"
     ]
    }
   ],
   "source": [
    "print(\"# Naive Bayes + Count Vectors + 1-gram\")\n",
    "train_model(naive_bayes.BernoulliNB(), xtrain_count_1n, train_y, xvalid_count_1n)\n",
    "\n",
    "print(\"\\n# Naive Bayes + Count Vectors + 2-gram\")\n",
    "train_model(naive_bayes.BernoulliNB(), xtrain_count_2n, train_y, xvalid_count_2n)\n",
    "\n",
    "print(\"\\n# Naive Bayes + Count Vectors + 3-gram\")\n",
    "train_model(naive_bayes.BernoulliNB(), xtrain_count_3n, train_y, xvalid_count_3n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T22:40:14.142268Z",
     "start_time": "2019-07-17T22:40:14.111139Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Naive Bayes + TF-IDF + 1-gram\n",
      "Accuracy: 0.836\n",
      "Precision: 0.87\n",
      "Recall: 0.8146\n",
      "F1 Score: 0.8414\n",
      "\n",
      "# Naive Bayes + TF-IDF + 2-gram\n",
      "Accuracy: 0.753\n",
      "Precision: 0.82\n",
      "Recall: 0.7231\n",
      "F1 Score: 0.7685\n",
      "\n",
      "# Naive Bayes + TF-IDF + 3-gram\n",
      "Accuracy: 0.609\n",
      "Precision: 0.85\n",
      "Recall: 0.5735\n",
      "F1 Score: 0.6849\n"
     ]
    }
   ],
   "source": [
    "print(\"# Naive Bayes + TF-IDF + 1-gram\")\n",
    "train_model(naive_bayes.BernoulliNB(), xtrain_tfidf_1n, train_y, xvalid_tfidf_1n)\n",
    "\n",
    "print(\"\\n# Naive Bayes + TF-IDF + 2-gram\")\n",
    "train_model(naive_bayes.BernoulliNB(), xtrain_tfidf_2n, train_y, xvalid_tfidf_2n)\n",
    "\n",
    "print(\"\\n# Naive Bayes + TF-IDF + 3-gram\")\n",
    "train_model(naive_bayes.BernoulliNB(), xtrain_tfidf_3n, train_y, xvalid_tfidf_3n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T22:45:03.233145Z",
     "start_time": "2019-07-17T22:45:03.164318Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Naive Bayes + 1234 + Word2vec\n",
      "Accuracy: 0.549\n",
      "Precision: 0.562\n",
      "Recall: 0.5478\n",
      "F1 Score: 0.5548\n",
      "\n",
      "# Naive Bayes + 14 + Word2vec\n",
      "Accuracy: 0.545\n",
      "Precision: 0.428\n",
      "Recall: 0.5587\n",
      "F1 Score: 0.4847\n"
     ]
    }
   ],
   "source": [
    "print(\"# Naive Bayes + 1234 + Word2vec\")\n",
    "train_model(naive_bayes.BernoulliNB(), train_vec, train_y, valid_vec)\n",
    "\n",
    "print(\"\\n# Naive Bayes + 14 + Word2vec\")\n",
    "train_model(naive_bayes.BernoulliNB(), train_w2v_vec, train_y, valid_w2v_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T21:43:50.499936Z",
     "start_time": "2019-07-17T21:43:42.219109Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM + Count Vectors + 1-gram\n",
      "[LibSVM]Accuracy: 0.772\n",
      "Precision: 0.798\n",
      "Recall: 0.7586\n",
      "F1 Score: 0.7778\n",
      "\n",
      "SVM + Count Vectors + 2-gram\n",
      "[LibSVM]Accuracy: 0.538\n",
      "Precision: 0.972\n",
      "Recall: 0.5203\n",
      "F1 Score: 0.6778\n",
      "\n",
      "SVM + Count Vectors + 3-gram\n",
      "[LibSVM]Accuracy: 0.535\n",
      "Precision: 0.97\n",
      "Recall: 0.5187\n",
      "F1 Score: 0.676\n"
     ]
    }
   ],
   "source": [
    "print(\"SVM + Count Vectors + 1-gram\")\n",
    "train_model(svm.SVC(verbose=True), xtrain_count_1n, train_y, xvalid_count_1n)\n",
    "\n",
    "print(\"\\nSVM + Count Vectors + 2-gram\")\n",
    "train_model(svm.SVC(verbose=True), xtrain_count_2n, train_y, xvalid_count_2n)\n",
    "\n",
    "print(\"\\nSVM + Count Vectors + 3-gram\")\n",
    "train_model(svm.SVC(verbose=True), xtrain_count_3n, train_y, xvalid_count_3n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T21:09:13.802851Z",
     "start_time": "2019-07-17T21:09:05.365939Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM + TF-IDF + 1-gram\n",
      "Accuracy: 0.798\n",
      "Precision: 0.914\n",
      "Recall: 0.7419\n",
      "F1 Score: 0.819\n",
      "\n",
      "SVM + TF-IDF + 2-gram\n",
      "Accuracy: 0.556\n",
      "Precision: 0.986\n",
      "Recall: 0.5301\n",
      "F1 Score: 0.6895\n",
      "\n",
      "SVM + TF-IDF + 3-gram\n",
      "Accuracy: 0.513\n",
      "Precision: 0.996\n",
      "Recall: 0.5066\n",
      "F1 Score: 0.6716\n"
     ]
    }
   ],
   "source": [
    "print(\"SVM + TF-IDF + 1-gram\")\n",
    "train_model(svm.SVC(), xtrain_tfidf_1n, train_y, xvalid_tfidf_1n)\n",
    "\n",
    "print(\"\\nSVM + TF-IDF + 2-gram\")\n",
    "train_model(svm.SVC(), xtrain_tfidf_2n, train_y, xvalid_tfidf_2n)\n",
    "\n",
    "print(\"\\nSVM + TF-IDF + 3-gram\")\n",
    "train_model(svm.SVC(), xtrain_tfidf_3n, train_y, xvalid_tfidf_3n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T22:46:06.953303Z",
     "start_time": "2019-07-17T22:45:48.234911Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM + 1234 + Word2vec\n",
      "Accuracy: 0.553\n",
      "Precision: 0.33\n",
      "Recall: 0.5957\n",
      "F1 Score: 0.4247\n",
      "\n",
      "SVM + 14 + Word2vec\n",
      "Accuracy: 0.514\n",
      "Precision: 0.094\n",
      "Recall: 0.5875\n",
      "F1 Score: 0.1621\n"
     ]
    }
   ],
   "source": [
    "print(\"SVM + 1234 + Word2vec\")\n",
    "train_model(svm.SVC(), train_vec, train_y, valid_vec)\n",
    "\n",
    "print(\"\\nSVM + 14 + Word2vec\")\n",
    "train_model(svm.SVC(), train_w2v_vec, train_y, valid_w2v_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T21:09:14.036219Z",
     "start_time": "2019-07-17T21:09:13.942273Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression + Count Vectors + 1-gram\n",
      "Accuracy: 0.833\n",
      "Precision: 0.846\n",
      "Recall: 0.8246\n",
      "F1 Score: 0.8351\n",
      "\n",
      "Logistic Regression + Count Vectors + 2-gram\n",
      "Accuracy: 0.74\n",
      "Precision: 0.76\n",
      "Recall: 0.7308\n",
      "F1 Score: 0.7451\n",
      "\n",
      "Logistic Regression + Count Vectors + 3-gram\n",
      "Accuracy: 0.619\n",
      "Precision: 0.838\n",
      "Recall: 0.5828\n",
      "F1 Score: 0.6874\n"
     ]
    }
   ],
   "source": [
    "print(\"Logistic Regression + Count Vectors + 1-gram\")\n",
    "train_model(linear_model.LogisticRegression(), xtrain_count_1n, train_y, xvalid_count_1n)\n",
    "\n",
    "print(\"\\nLogistic Regression + Count Vectors + 2-gram\")\n",
    "train_model(linear_model.LogisticRegression(), xtrain_count_2n, train_y, xvalid_count_2n)\n",
    "\n",
    "print(\"\\nLogistic Regression + Count Vectors + 3-gram\")\n",
    "train_model(linear_model.LogisticRegression(), xtrain_count_3n, train_y, xvalid_count_3n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T21:09:14.245312Z",
     "start_time": "2019-07-17T21:09:14.194310Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression + TF-IDF + 1-gram\n",
      "Accuracy: 0.856\n",
      "Precision: 0.862\n",
      "Recall: 0.8518\n",
      "F1 Score: 0.8569\n",
      "\n",
      "Logistic Regression + TF-IDF + 2-gram\n",
      "Accuracy: 0.536\n",
      "Precision: 0.696\n",
      "Recall: 0.5273\n",
      "F1 Score: 0.6\n",
      "\n",
      "Logistic Regression + TF-IDF + 3-gram\n",
      "Accuracy: 0.617\n",
      "Precision: 0.834\n",
      "Recall: 0.5816\n",
      "F1 Score: 0.6853\n"
     ]
    }
   ],
   "source": [
    "print(\"Logistic Regression + TF-IDF + 1-gram\")\n",
    "train_model(linear_model.LogisticRegression(), xtrain_tfidf_1n, train_y, xvalid_tfidf_1n)\n",
    "\n",
    "print(\"\\nLogistic Regression + TF-IDF + 2-gram\")\n",
    "train_model(linear_model.LogisticRegression(), xtrain_tfidf_2n, train_y, xvalid_tfidf_3n)\n",
    "\n",
    "print(\"\\nLogistic Regression + TF-IDF + 3-gram\")\n",
    "train_model(linear_model.LogisticRegression(), xtrain_tfidf_3n, train_y, xvalid_tfidf_3n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T22:46:27.320118Z",
     "start_time": "2019-07-17T22:46:27.048221Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression + 1234 + Word2vec\n",
      "Accuracy: 0.551\n",
      "Precision: 0.59\n",
      "Recall: 0.5473\n",
      "F1 Score: 0.5679\n",
      "\n",
      "Logistic Regression + 14 + Word2vec\n",
      "Accuracy: 0.584\n",
      "Precision: 0.586\n",
      "Recall: 0.5837\n",
      "F1 Score: 0.5848\n"
     ]
    }
   ],
   "source": [
    "print(\"Logistic Regression + 1234 + Word2vec\")\n",
    "train_model(linear_model.LogisticRegression(), train_vec, train_y, valid_vec)\n",
    "\n",
    "print(\"\\nLogistic Regression + 14 + Word2vec\")\n",
    "train_model(linear_model.LogisticRegression(), train_w2v_vec, train_y, valid_w2v_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training (DNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-18T01:59:14.868381Z",
     "start_time": "2019-07-18T01:59:14.865191Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import text, sequence\n",
    "from keras import layers, models, optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-18T02:28:01.215530Z",
     "start_time": "2019-07-18T02:28:01.212801Z"
    }
   },
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 128 \n",
    "EMBEDDING_DIM = 300 \n",
    "VALIDATION_SPLIT = 0.15\n",
    "TEST_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-18T01:59:16.343926Z",
     "start_time": "2019-07-18T01:59:15.801791Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 18340 unique tokens.\n",
      "Shape of data tensor: (5000, 128)\n",
      "Shape of label tensor: (5000, 2)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "\n",
    "# One-hot\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_sp['Token'])\n",
    "sequences = tokenizer.texts_to_sequences(train_sp['Token'])\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "labels = to_categorical(np.asarray(train_sp['Sentiment']))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-18T01:59:16.741823Z",
     "start_time": "2019-07-18T01:59:16.734071Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train rows: 3249\n",
      "valid rows: 751\n",
      "test rows: 1000\n"
     ]
    }
   ],
   "source": [
    "p1 = int(len(data)*(1-VALIDATION_SPLIT-TEST_SPLIT))\n",
    "p2 = int(len(data)*(1-TEST_SPLIT))\n",
    "x_train = data[:p1]\n",
    "y_train = labels[:p1]\n",
    "x_val = data[p1:p2]\n",
    "y_val = labels[p1:p2]\n",
    "x_test = data[p2:]\n",
    "y_test = labels[p2:]\n",
    "print('train rows: '+str(len(x_train)))\n",
    "print('valid rows: '+str(len(x_val)))\n",
    "print('test rows: '+str(len(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-18T01:59:17.318825Z",
     "start_time": "2019-07-18T01:59:17.204055Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_37 (Embedding)     (None, 128, 300)          5502300   \n",
      "_________________________________________________________________\n",
      "dropout_28 (Dropout)         (None, 128, 300)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_19 (Conv1D)           (None, 126, 250)          225250    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling (None, 42, 250)           0         \n",
      "_________________________________________________________________\n",
      "flatten_16 (Flatten)         (None, 10500)             0         \n",
      "_________________________________________________________________\n",
      "dense_57 (Dense)             (None, 300)               3150300   \n",
      "_________________________________________________________________\n",
      "dense_58 (Dense)             (None, 2)                 602       \n",
      "=================================================================\n",
      "Total params: 8,878,452\n",
      "Trainable params: 8,878,452\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense, Input, Flatten, Dropout\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Sequential\n",
    "\n",
    "cnn = Sequential()\n",
    "cnn.add(Embedding(len(word_index) + 1, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH))\n",
    "cnn.add(Dropout(0.2))\n",
    "cnn.add(Conv1D(250, 3, padding='valid', activation='relu', strides=1))\n",
    "cnn.add(MaxPooling1D(3))\n",
    "cnn.add(Flatten())\n",
    "cnn.add(Dense(EMBEDDING_DIM, activation='relu'))\n",
    "cnn.add(Dense(labels.shape[1], activation='softmax'))\n",
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-18T02:00:06.596002Z",
     "start_time": "2019-07-18T01:59:17.508725Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3249 samples, validate on 751 samples\n",
      "Epoch 1/5\n",
      "3249/3249 [==============================] - 13s 4ms/step - loss: 0.7874 - acc: 0.5454 - val_loss: 0.6612 - val_acc: 0.5180\n",
      "Epoch 2/5\n",
      "3249/3249 [==============================] - 9s 3ms/step - loss: 0.4608 - acc: 0.7833 - val_loss: 0.4083 - val_acc: 0.8162\n",
      "Epoch 3/5\n",
      "3249/3249 [==============================] - 9s 3ms/step - loss: 0.1641 - acc: 0.9418 - val_loss: 0.6337 - val_acc: 0.7483\n",
      "Epoch 4/5\n",
      "3249/3249 [==============================] - 9s 3ms/step - loss: 0.0636 - acc: 0.9797 - val_loss: 0.5335 - val_acc: 0.8176\n",
      "Epoch 5/5\n",
      "3249/3249 [==============================] - 9s 3ms/step - loss: 0.0144 - acc: 0.9972 - val_loss: 0.7147 - val_acc: 0.8056\n",
      "1000/1000 [==============================] - 1s 820us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.655867104113102, 0.8]"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CNN + One-hot\n",
    "cnn.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "cnn.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=5, batch_size=128)\n",
    "cnn.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-18T02:00:07.516534Z",
     "start_time": "2019-07-18T02:00:07.210953Z"
    }
   },
   "outputs": [],
   "source": [
    "# Word2vec\n",
    "from keras.utils import plot_model\n",
    "from keras.layers import Embedding\n",
    "\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items(): \n",
    "    if str(word) in w2vmodel:\n",
    "        embedding_matrix[i] = np.asarray(w2vmodel[str(word)],dtype='float32')\n",
    "        \n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-18T02:00:43.576906Z",
     "start_time": "2019-07-18T02:00:07.852660Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_38 (Embedding)     (None, 128, 300)          5502300   \n",
      "_________________________________________________________________\n",
      "dropout_29 (Dropout)         (None, 128, 300)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_20 (Conv1D)           (None, 126, 250)          225250    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling (None, 42, 250)           0         \n",
      "_________________________________________________________________\n",
      "flatten_17 (Flatten)         (None, 10500)             0         \n",
      "_________________________________________________________________\n",
      "dense_59 (Dense)             (None, 300)               3150300   \n",
      "_________________________________________________________________\n",
      "dense_60 (Dense)             (None, 2)                 602       \n",
      "=================================================================\n",
      "Total params: 8,878,452\n",
      "Trainable params: 3,376,152\n",
      "Non-trainable params: 5,502,300\n",
      "_________________________________________________________________\n",
      "Train on 3249 samples, validate on 751 samples\n",
      "Epoch 1/5\n",
      "3249/3249 [==============================] - 9s 3ms/step - loss: 0.9663 - acc: 0.5355 - val_loss: 0.6494 - val_acc: 0.6272\n",
      "Epoch 2/5\n",
      "3249/3249 [==============================] - 6s 2ms/step - loss: 0.6516 - acc: 0.6291 - val_loss: 0.6301 - val_acc: 0.6605\n",
      "Epoch 3/5\n",
      "3249/3249 [==============================] - 6s 2ms/step - loss: 0.6123 - acc: 0.6722 - val_loss: 0.6055 - val_acc: 0.6778\n",
      "Epoch 4/5\n",
      "3249/3249 [==============================] - 6s 2ms/step - loss: 0.5668 - acc: 0.7045 - val_loss: 0.5937 - val_acc: 0.6738\n",
      "Epoch 5/5\n",
      "3249/3249 [==============================] - 6s 2ms/step - loss: 0.5217 - acc: 0.7421 - val_loss: 0.6381 - val_acc: 0.6165\n",
      "1000/1000 [==============================] - 1s 728us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6826513829231262, 0.604]"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CNN + Word2vec\n",
    "from keras.layers import Dense, Input, Flatten, Dropout\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Sequential\n",
    "\n",
    "cnn_w2v = Sequential()\n",
    "cnn_w2v.add(embedding_layer)\n",
    "cnn_w2v.add(Dropout(0.2))\n",
    "cnn_w2v.add(Conv1D(250, 3, padding='valid', activation='relu', strides=1))\n",
    "cnn_w2v.add(MaxPooling1D(3))\n",
    "cnn_w2v.add(Flatten())\n",
    "cnn_w2v.add(Dense(EMBEDDING_DIM, activation='relu'))\n",
    "cnn_w2v.add(Dense(labels.shape[1], activation='softmax'))\n",
    "cnn_w2v.summary()\n",
    "\n",
    "cnn_w2v.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "cnn_w2v.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=5, batch_size=128)\n",
    "cnn_w2v.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T23:01:03.811073Z",
     "start_time": "2019-07-17T23:01:03.783953Z"
    }
   },
   "source": [
    "## LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-18T02:02:03.407333Z",
     "start_time": "2019-07-18T02:00:43.993154Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_39 (Embedding)     (None, 128, 300)          5502300   \n",
      "_________________________________________________________________\n",
      "dropout_30 (Dropout)         (None, 128, 300)          0         \n",
      "_________________________________________________________________\n",
      "lstm_20 (LSTM)               (None, 128)               219648    \n",
      "_________________________________________________________________\n",
      "dense_61 (Dense)             (None, 1)                 129       \n",
      "_________________________________________________________________\n",
      "dense_62 (Dense)             (None, 300)               600       \n",
      "_________________________________________________________________\n",
      "dense_63 (Dense)             (None, 2)                 602       \n",
      "=================================================================\n",
      "Total params: 5,723,279\n",
      "Trainable params: 5,723,279\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 3249 samples, validate on 751 samples\n",
      "Epoch 1/5\n",
      "3249/3249 [==============================] - 18s 6ms/step - loss: 0.6933 - acc: 0.5011 - val_loss: 0.6853 - val_acc: 0.5007\n",
      "Epoch 2/5\n",
      "3249/3249 [==============================] - 14s 4ms/step - loss: 0.6299 - acc: 0.7045 - val_loss: 0.5999 - val_acc: 0.7417\n",
      "Epoch 3/5\n",
      "3249/3249 [==============================] - 14s 4ms/step - loss: 0.4928 - acc: 0.8738 - val_loss: 0.5183 - val_acc: 0.8109\n",
      "Epoch 4/5\n",
      "3249/3249 [==============================] - 15s 5ms/step - loss: 0.3733 - acc: 0.9221 - val_loss: 0.4888 - val_acc: 0.8202\n",
      "Epoch 5/5\n",
      "3249/3249 [==============================] - 15s 5ms/step - loss: 0.2817 - acc: 0.9351 - val_loss: 0.4920 - val_acc: 0.8149\n",
      "1000/1000 [==============================] - 1s 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.49554140675067904, 0.807]"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import Dense, Input, Flatten, Dropout\n",
    "from keras.layers import LSTM, MaxPooling1D, Embedding\n",
    "from keras.models import Sequential\n",
    "\n",
    "lstm = Sequential()\n",
    "lstm.add(Embedding(len(word_index) + 1, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH))\n",
    "lstm.add(Dropout(0.2))\n",
    "lstm.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "lstm.add(Dense(1, activation='sigmoid'))\n",
    "lstm.add(Dense(EMBEDDING_DIM, activation='relu'))\n",
    "lstm.add(Dense(labels.shape[1], activation='softmax'))\n",
    "lstm.summary()\n",
    "\n",
    "lstm.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "lstm.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=5, batch_size=128)\n",
    "lstm.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-18T02:28:13.091592Z",
     "start_time": "2019-07-18T02:28:12.758131Z"
    }
   },
   "outputs": [],
   "source": [
    "# Word2vec\n",
    "from keras.utils import plot_model\n",
    "from keras.layers import Embedding\n",
    "\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items(): \n",
    "    if str(word) in w2vmodel:\n",
    "        embedding_matrix[i] = np.asarray(w2vmodel[str(word)],dtype='float32')\n",
    "        \n",
    "embedding_layer = Embedding(len(word_index) + 1, EMBEDDING_DIM, weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH, trainable=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-18T02:29:56.551280Z",
     "start_time": "2019-07-18T02:28:44.264613Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_43 (Embedding)     (None, 128, 300)          5502300   \n",
      "_________________________________________________________________\n",
      "dropout_42 (Dropout)         (None, 128, 300)          0         \n",
      "_________________________________________________________________\n",
      "lstm_33 (LSTM)               (None, 128)               219648    \n",
      "_________________________________________________________________\n",
      "dense_64 (Dense)             (None, 1)                 129       \n",
      "_________________________________________________________________\n",
      "dense_65 (Dense)             (None, 300)               600       \n",
      "_________________________________________________________________\n",
      "dense_66 (Dense)             (None, 2)                 602       \n",
      "=================================================================\n",
      "Total params: 5,723,279\n",
      "Trainable params: 220,979\n",
      "Non-trainable params: 5,502,300\n",
      "_________________________________________________________________\n",
      "Train on 3249 samples, validate on 751 samples\n",
      "Epoch 1/5\n",
      "3249/3249 [==============================] - 17s 5ms/step - loss: 0.6929 - acc: 0.5155 - val_loss: 0.6857 - val_acc: 0.5846\n",
      "Epoch 2/5\n",
      "3249/3249 [==============================] - 13s 4ms/step - loss: 0.6836 - acc: 0.5876 - val_loss: 0.7118 - val_acc: 0.5180\n",
      "Epoch 3/5\n",
      "3249/3249 [==============================] - 12s 4ms/step - loss: 0.6769 - acc: 0.5900 - val_loss: 0.6825 - val_acc: 0.6099\n",
      "Epoch 4/5\n",
      "3249/3249 [==============================] - 12s 4ms/step - loss: 0.6629 - acc: 0.6254 - val_loss: 0.6426 - val_acc: 0.6605\n",
      "Epoch 5/5\n",
      "3249/3249 [==============================] - 16s 5ms/step - loss: 0.6551 - acc: 0.6359 - val_loss: 0.6578 - val_acc: 0.6298\n",
      "1000/1000 [==============================] - 2s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6435657901763916, 0.654]"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CNN + Word2vec\n",
    "from keras.layers import Dense, Input, Flatten, Dropout\n",
    "from keras.layers import LSTM, MaxPooling1D, Embedding\n",
    "from keras.models import Sequential\n",
    "\n",
    "lstm_w2v = Sequential()\n",
    "lstm_w2v.add(embedding_layer)\n",
    "lstm_w2v.add(Dropout(0.2))\n",
    "lstm_w2v.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "lstm_w2v.add(Dense(1, activation='sigmoid'))\n",
    "lstm_w2v.add(Dense(EMBEDDING_DIM, activation='relu'))\n",
    "lstm_w2v.add(Dense(labels.shape[1], activation='softmax'))\n",
    "lstm_w2v.summary()\n",
    "\n",
    "lstm_w2v.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "lstm_w2v.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=5, batch_size=128)\n",
    "lstm_w2v.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-18T02:58:28.804280Z",
     "start_time": "2019-07-18T02:56:34.525278Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_78 (Embedding)     (None, 128, 300)          5502300   \n",
      "_________________________________________________________________\n",
      "dropout_72 (Dropout)         (None, 128, 300)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_55 (Conv1D)           (None, 126, 250)          225250    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling (None, 42, 250)           0         \n",
      "_________________________________________________________________\n",
      "lstm_58 (LSTM)               (None, 128)               194048    \n",
      "_________________________________________________________________\n",
      "dense_73 (Dense)             (None, 2)                 258       \n",
      "_________________________________________________________________\n",
      "dense_74 (Dense)             (None, 300)               900       \n",
      "_________________________________________________________________\n",
      "dense_75 (Dense)             (None, 2)                 602       \n",
      "=================================================================\n",
      "Total params: 5,923,358\n",
      "Trainable params: 5,923,358\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 3249 samples, validate on 751 samples\n",
      "Epoch 1/5\n",
      "3249/3249 [==============================] - 26s 8ms/step - loss: 0.6553 - acc: 0.6119 - val_loss: 0.5302 - val_acc: 0.7923\n",
      "Epoch 2/5\n",
      "3249/3249 [==============================] - 24s 7ms/step - loss: 0.3894 - acc: 0.8797 - val_loss: 0.4129 - val_acc: 0.8229\n",
      "Epoch 3/5\n",
      "3249/3249 [==============================] - 22s 7ms/step - loss: 0.2109 - acc: 0.9455 - val_loss: 0.4062 - val_acc: 0.8389\n",
      "Epoch 4/5\n",
      "3249/3249 [==============================] - 22s 7ms/step - loss: 0.1155 - acc: 0.9723 - val_loss: 1.4223 - val_acc: 0.6099\n",
      "Epoch 5/5\n",
      "3249/3249 [==============================] - 16s 5ms/step - loss: 0.1212 - acc: 0.9701 - val_loss: 0.5868 - val_acc: 0.8123\n",
      "1000/1000 [==============================] - 1s 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5607659213542938, 0.818]"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CNN + LSTM\n",
    "from keras.layers import Dense, Input, Flatten, Dropout\n",
    "from keras.layers import LSTM, Conv1D, MaxPooling1D, Embedding, TimeDistributed\n",
    "from keras.models import Sequential\n",
    "\n",
    "hybrid = Sequential()\n",
    "hybrid.add(Embedding(len(word_index) + 1, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH))\n",
    "hybrid.add(Dropout(0.2))\n",
    "hybrid.add(Conv1D(250, 3, activation='relu'))\n",
    "hybrid.add(MaxPooling1D(pool_size=3))\n",
    "hybrid.add(LSTM(128))\n",
    "hybrid.add(Dense(2, activation='sigmoid'))\n",
    "hybrid.add(Dense(EMBEDDING_DIM, activation='relu'))\n",
    "hybrid.add(Dense(labels.shape[1], activation='softmax'))\n",
    "hybrid.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "hybrid.summary()\n",
    "\n",
    "hybrid.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "hybrid.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=5, batch_size=128)\n",
    "hybrid.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimisation (CV: Parameters & Features Selection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reflection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
