{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T08:40:11.727634Z",
     "start_time": "2019-07-03T08:40:11.720100Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt  \n",
    "import seaborn as sns  \n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords                                    # Stopwords corpus\n",
    "from nltk.stem import PorterStemmer                                  # Stemmer\n",
    "\n",
    "from sklearn import model_selection, preprocessing, linear_model, metrics, svm\n",
    "from sklearn import decomposition, ensemble\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer          # For Bag of words\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer          # For TF-IDF\n",
    "from gensim.models import Word2Vec                                   # For Word2Vec\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T08:40:12.655510Z",
     "start_time": "2019-07-03T08:40:12.650247Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test01.ipynb', 'test.csv', 'readme.txt', 'train.csv', '.ipynb_checkpoints']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T08:40:35.112934Z",
     "start_time": "2019-07-03T08:40:14.302135Z"
    }
   },
   "outputs": [],
   "source": [
    "# Input Dataset\n",
    "test_file = pd.read_csv('test.csv', header=None)\n",
    "train_file = pd.read_csv('train.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T08:40:40.412570Z",
     "start_time": "2019-07-03T08:40:35.263914Z"
    }
   },
   "outputs": [],
   "source": [
    "# Merge title and content\n",
    "test_file['Comment'] = test_file[1]+' '+test_file[2]\n",
    "train_file['Comment'] = train_file[1]+' '+train_file[2]\n",
    "test_file = test_file.drop(columns=[1,2])\n",
    "train_file = train_file.drop(columns=[1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T08:40:41.932900Z",
     "start_time": "2019-07-03T08:40:40.499362Z"
    }
   },
   "outputs": [],
   "source": [
    "# Edit label\n",
    "train_file[0] = train_file[0].map(lambda x: x-1)\n",
    "test_file[0] = test_file[0].map(lambda x: x-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T08:40:42.019877Z",
     "start_time": "2019-07-03T08:40:42.008893Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>Comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Stuning even for the non-gamer This sound trac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>The best soundtrack ever to anything. I'm read...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Amazing! This soundtrack is my favorite music ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Excellent Soundtrack I truly like this soundtr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Remember, Pull Your Jaw Off The Floor After He...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0                                            Comment\n",
       "0  1  Stuning even for the non-gamer This sound trac...\n",
       "1  1  The best soundtrack ever to anything. I'm read...\n",
       "2  1  Amazing! This soundtrack is my favorite music ...\n",
       "3  1  Excellent Soundtrack I truly like this soundtr...\n",
       "4  1  Remember, Pull Your Jaw Off The Floor After He..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_file[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T08:40:42.097117Z",
     "start_time": "2019-07-03T08:40:42.089118Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>Comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Great CD My lovely Pat has one of the GREAT vo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>One of the best game music soundtracks - for a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Batteries died within a year ... I bought this...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>works fine, but Maha Energy is better Check ou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Great for the non-audiophile Reviewed quite a ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0                                            Comment\n",
       "0  1  Great CD My lovely Pat has one of the GREAT vo...\n",
       "1  1  One of the best game music soundtracks - for a...\n",
       "2  0  Batteries died within a year ... I bought this...\n",
       "3  1  works fine, but Maha Energy is better Check ou...\n",
       "4  1  Great for the non-audiophile Reviewed quite a ..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_file[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T08:40:42.188009Z",
     "start_time": "2019-07-03T08:40:42.181846Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'couldn', \"she's\", \"wasn't\", 'all', 'some', 'can', 'is', 'or', 'will', 'needn', 'him', 'ours', 'didn', 'should', \"weren't\", 'for', 'ain', 'now', 'under', 's', 'whom', 'during', 'above', 'shouldn', 'themselves', \"you've\", 'were', \"hasn't\", 'these', 'me', 'off', 'to', 'not', 'down', 'the', \"aren't\", 'very', 'her', 'doing', 'yours', 'and', \"it's\", 'our', 'so', 'most', 'hadn', 'don', 'out', 'up', 'are', 'other', 'your', 'only', 'them', 'as', 'below', 'each', 'just', 'than', \"couldn't\", 'once', \"that'll\", 'himself', 'those', 'has', 'y', 'there', 'nor', \"doesn't\", \"didn't\", 'where', 'why', 'she', 'same', 'both', 'own', 'have', 'they', \"don't\", 'wasn', 'against', 'such', 'again', 'from', \"wouldn't\", 'you', 'being', 'mightn', 'then', 'on', 'over', 'before', 'its', 'yourself', 'do', 'through', 'isn', 'by', 'he', 'shan', 'what', 'be', \"shouldn't\", \"you'd\", \"shan't\", 'was', \"isn't\", 'does', 'how', 'if', 'no', 'am', 're', 'a', 'having', 'aren', 'his', 'at', \"you're\", 'herself', 'o', \"hadn't\", 'with', 'i', 'further', 'in', 'd', 'did', 'm', 'wouldn', 'here', 'while', 'itself', \"won't\", 'theirs', 'hers', 'mustn', 'weren', 'because', 'll', 'few', 'but', 'after', 'myself', 'that', 'this', 'won', 'too', 'ourselves', 'who', 'hasn', 'been', 'which', 'yourselves', 'doesn', \"should've\", 'any', 'an', \"haven't\", 'into', 'it', 't', 'haven', 'we', 'of', 'when', 'more', \"mustn't\", 'about', 'until', 'between', 'their', \"needn't\", \"mightn't\", 've', 'my', 'ma', \"you'll\", 'had'}\n"
     ]
    }
   ],
   "source": [
    "stop = set(stopwords.words('english'))\n",
    "print(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T08:40:42.269426Z",
     "start_time": "2019-07-03T08:40:42.263039Z"
    }
   },
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "def prepr(X):\n",
    "    temp =[]\n",
    "    snow = nltk.stem.SnowballStemmer('english')\n",
    "    for sentence in tqdm(X):\n",
    "        # Converting to lowercase\n",
    "        sentence = str(sentence).lower()\n",
    "        # Removing HTML tags\n",
    "        cleanr = re.compile('<.*?>')\n",
    "        sentence = re.sub(cleanr, ' ', sentence)\n",
    "        sentence = re.sub(r'[?|!|\\'|\"|#]',r'',sentence)\n",
    "        # Removing Punctuations\n",
    "        sentence = re.sub(r'[.|,|)|(|\\|/]',r' ',sentence)\n",
    "        # Stemming and removing stopwords\n",
    "        words = ' '.join([snow.stem(word) for word in sentence.split() if word not in stopwords.words('english')])\n",
    "        temp.append(words)\n",
    "\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T08:42:55.137384Z",
     "start_time": "2019-07-03T08:40:42.345850Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [02:12<00:00, 75.31it/s]\n"
     ]
    }
   ],
   "source": [
    "# 10000 size sample\n",
    "train_s_X = prepr(train_file['Comment'][:10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T08:42:55.328427Z",
     "start_time": "2019-07-03T08:42:55.324398Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['stune even non-gam sound track beauti paint seneri mind well would recomend even peopl hate vid game music play game chrono cross game ever play best music back away crude keyboard take fresher step grate guitar soul orchestra would impress anyon care listen ^_^',\n",
       " 'best soundtrack ever anyth im read lot review say best game soundtrack figur id write review disagre bit opinino yasunori mitsuda ultim masterpiec music timeless im listen year beauti simpli refus fade price tag pretti stagger must say go buy cd much money one feel would worth everi penni']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_s_X[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T08:42:55.493107Z",
     "start_time": "2019-07-03T08:42:55.468190Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>stune even non-gam sound track beauti paint se...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>best soundtrack ever anyth im read lot review ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>amaz soundtrack favorit music time hand intens...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>excel soundtrack truli like soundtrack enjoy v...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rememb pull jaw floor hear youv play game know...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>absolut masterpiec quit sure actual take time ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>buyer bewar self-publish book want know why--r...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>glorious stori love whisper wick saint stori a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>five star book finish read whisper wick saint ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>whisper wick saint easi read book made want ke...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  stune even non-gam sound track beauti paint se...      1\n",
       "1  best soundtrack ever anyth im read lot review ...      1\n",
       "2  amaz soundtrack favorit music time hand intens...      1\n",
       "3  excel soundtrack truli like soundtrack enjoy v...      1\n",
       "4  rememb pull jaw floor hear youv play game know...      1\n",
       "5  absolut masterpiec quit sure actual take time ...      1\n",
       "6  buyer bewar self-publish book want know why--r...      0\n",
       "7  glorious stori love whisper wick saint stori a...      1\n",
       "8  five star book finish read whisper wick saint ...      1\n",
       "9  whisper wick saint easi read book made want ke...      1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDF = pd.DataFrame()\n",
    "trainDF['text'] = train_s_X\n",
    "trainDF['label'] = list(train_file[0][:10000])\n",
    "trainDF.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T08:42:55.662039Z",
     "start_time": "2019-07-03T08:42:55.650266Z"
    }
   },
   "outputs": [],
   "source": [
    "# split the dataset into training and validation datasets \n",
    "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(trainDF['text'], trainDF['label'])\n",
    "\n",
    "# label encode the target variable \n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "valid_y = encoder.fit_transform(valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T08:43:07.259847Z",
     "start_time": "2019-07-03T08:42:55.839722Z"
    }
   },
   "outputs": [],
   "source": [
    "# word level tf-idf\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
    "tfidf_vect.fit(trainDF['text'])\n",
    "xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
    "xvalid_tfidf =  tfidf_vect.transform(valid_x)\n",
    "\n",
    "# ngram level tf-idf \n",
    "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "tfidf_vect_ngram.fit(trainDF['text'])\n",
    "xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(train_x)\n",
    "xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(valid_x)\n",
    "\n",
    "# characters level tf-idf\n",
    "tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "tfidf_vect_ngram_chars.fit(trainDF['text'])\n",
    "xtrain_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(train_x) \n",
    "xvalid_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(valid_x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training (Traditional ML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T08:43:07.647572Z",
     "start_time": "2019-07-03T08:43:07.643439Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_model(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net=False):\n",
    "    # fit the training dataset on the classifier\n",
    "    classifier.fit(feature_vector_train, label)\n",
    "    \n",
    "    # predict the labels on validation dataset\n",
    "    predictions = classifier.predict(feature_vector_valid)\n",
    "    \n",
    "    if is_neural_net:\n",
    "        predictions = predictions.argmax(axis=-1)\n",
    "    \n",
    "    return metrics.accuracy_score(predictions, valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T08:43:08.376489Z",
     "start_time": "2019-07-03T08:43:08.013492Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR, WordLevel TF-IDF:  0.8552\n",
      "LR, N-Gram Vectors:  0.7864\n",
      "LR, CharLevel Vectors:  0.8256\n"
     ]
    }
   ],
   "source": [
    "# Linear Classifier on Word Level TF IDF Vectors\n",
    "accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print(\"LR, WordLevel TF-IDF: \", accuracy)\n",
    "\n",
    "# Linear Classifier on Ngram Level TF IDF Vectors\n",
    "accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "print(\"LR, N-Gram Vectors: \", accuracy)\n",
    "\n",
    "# Linear Classifier on Character Level TF IDF Vectors\n",
    "accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars)\n",
    "print(\"LR, CharLevel Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T08:46:52.954520Z",
     "start_time": "2019-07-03T08:43:08.769072Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM, WordLevel TF-IDF:  0.5088\n",
      "SVM, N-Gram Vectors:  0.5088\n",
      "SVM, CharLevel Vectors:  0.5088\n"
     ]
    }
   ],
   "source": [
    "# SVM on on Word Level TF IDF Vectors\n",
    "accuracy = train_model(svm.SVC(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print(\"SVM, WordLevel TF-IDF: \", accuracy)\n",
    "\n",
    "# SVM on Ngram Level TF IDF Vectors\n",
    "accuracy = train_model(svm.SVC(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "print(\"SVM, N-Gram Vectors: \", accuracy)\n",
    "\n",
    "# SVM on Character Level TF IDF Vectors\n",
    "accuracy = train_model(svm.SVC(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars)\n",
    "print(\"SVM, CharLevel Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training (NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T08:46:55.154781Z",
     "start_time": "2019-07-03T08:46:53.351158Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import text, sequence\n",
    "from keras import layers, models, optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T08:40:01.206249Z",
     "start_time": "2019-07-03T08:39:34.187Z"
    }
   },
   "outputs": [],
   "source": [
    "# Unigram \n",
    "count_vect = CountVectorizer(max_features=10000) # top 10000 most frequently repeated words \n",
    "small_sample_1N = count_vect.fit_transform(small_sample)\n",
    "print(small_sample_1N[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T08:40:01.208561Z",
     "start_time": "2019-07-03T08:39:34.189Z"
    }
   },
   "outputs": [],
   "source": [
    "# Bigram\n",
    "count_vect = CountVectorizer(ngram_range=(1,2), max_features=10000)\n",
    "small_sample_2N = count_vect.fit_transform(small_sample)\n",
    "print(small_sample_2N[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T08:40:01.210214Z",
     "start_time": "2019-07-03T08:39:34.192Z"
    }
   },
   "outputs": [],
   "source": [
    "# Trigram\n",
    "count_vect = CountVectorizer(ngram_range=(1,3), max_features=10000)\n",
    "small_sample_3N = count_vect.fit_transform(small_sample)\n",
    "print(small_sample_3N[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T08:40:01.211537Z",
     "start_time": "2019-07-03T08:39:34.197Z"
    }
   },
   "outputs": [],
   "source": [
    "# TF-IDF\n",
    "tf_idf = TfidfVectorizer(max_features=10000)\n",
    "small_sample_TF = tf_idf.fit_transform(small_sample)\n",
    "print(small_sample_TF[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T08:40:01.213620Z",
     "start_time": "2019-07-03T08:39:34.199Z"
    }
   },
   "outputs": [],
   "source": [
    "# Word2Vec\n",
    "small_sample_splitted = []\n",
    "for row in tqdm(small_sample): \n",
    "    small_sample_splitted.append([word for word in row.split()])                    # splitting words\n",
    "\n",
    "small_sample_w2v = Word2Vec(small_sample_splitted, min_count=5, size=50, workers=8) # 50 hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T08:40:01.214964Z",
     "start_time": "2019-07-03T08:39:34.201Z"
    }
   },
   "outputs": [],
   "source": [
    "# Average Word2Vec\n",
    "small_sample_avg_w2v = []\n",
    "for row in tqdm(small_sample_splitted):\n",
    "    vec = np.zeros(50)\n",
    "    count = 0\n",
    "    for word in row:\n",
    "        try:\n",
    "            vec += small_sample_w2v[word]\n",
    "            count += 1\n",
    "        except:\n",
    "            pass\n",
    "    small_sample_avg_w2v.append(vec/count)\n",
    "    \n",
    "print(small_sample_avg_w2v[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T08:40:01.216598Z",
     "start_time": "2019-07-03T08:39:34.203Z"
    }
   },
   "outputs": [],
   "source": [
    "# TF-IDF Word2Vec\n",
    "\n",
    "small_sample_TF_w2v = []\n",
    "small_sample_TF_data = small_sample_TF.toarray()\n",
    "i = 0\n",
    "for row in tqdm(small_sample_splitted):\n",
    "    vec = [0 for i in range(50)]\n",
    "    \n",
    "    temp_tf = []\n",
    "    for val in small_sample_TF_data[i]:\n",
    "        if val != 0:\n",
    "            temp_tf.append(val)\n",
    "    \n",
    "    count = 0\n",
    "    tf_idf_sum = 0\n",
    "    for word in row:\n",
    "        try:\n",
    "            count += 1\n",
    "            tf_idf_sum += temp_tf[count-1]\n",
    "            vec += (temp_tf[count-1] * small_sample_w2v[word])\n",
    "        except:\n",
    "            pass\n",
    "    #print(tf_idf_sum, vec)\n",
    "    vec = float(1 / tf_idf_sum) * vec\n",
    "    small_sample_TF_w2v.append(vec)\n",
    "    i = i + 1\n",
    "\n",
    "print(small_sample_TF_w2v[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T08:40:01.217720Z",
     "start_time": "2019-07-03T08:39:34.209Z"
    }
   },
   "outputs": [],
   "source": [
    "BoW1N_DF = pd.DataFrame()\n",
    "BoW1N_DF['label'] = small_sample_label\n",
    "BoW1N_DF['text'] = list(small_sample_1N)\n",
    "\n",
    "BoW2N_DF = pd.DataFrame()\n",
    "BoW2N_DF['label'] = small_sample_label\n",
    "BoW2N_DF['text'] = list(small_sample_2N)\n",
    "\n",
    "BoW3N_DF = pd.DataFrame()\n",
    "BoW3N_DF['label'] = small_sample_label\n",
    "BoW3N_DF['text'] = list(small_sample_3N)\n",
    "\n",
    "tf_DF = pd.DataFrame()\n",
    "tf_DF['label'] = small_sample_label\n",
    "tf_DF['text'] = list(small_sample_TF)\n",
    "\n",
    "w2v_DF = pd.DataFrame()\n",
    "w2v_DF['label'] = small_sample_label\n",
    "w2v_DF['text'] = small_sample_avg_w2v\n",
    "\n",
    "tf_w2v_DF = pd.DataFrame()\n",
    "tf_w2v_DF['label'] = small_sample_label\n",
    "tf_w2v_DF['text'] = small_sample_TF_w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T08:40:01.219006Z",
     "start_time": "2019-07-03T08:39:34.211Z"
    }
   },
   "outputs": [],
   "source": [
    "encoder = preprocessing.LabelEncoder()\n",
    "\n",
    "BoW1N_train_x, BoW1N_valid_x, BoW1N_train_y, BoW1N_valid_y = model_selection.train_test_split(BoW1N_DF['text'], BoW1N_DF['label'])\n",
    "BoW1N_train_y = encoder.fit_transform(BoW1N_train_y)\n",
    "BoW1N_valid_y = encoder.fit_transform(BoW1N_valid_y)\n",
    "\n",
    "BoW2N_train_x, BoW2N_valid_x, BoW2N_train_y, BoW2N_valid_y = model_selection.train_test_split(BoW2N_DF['text'], BoW2N_DF['label'])\n",
    "BoW2N_train_y = encoder.fit_transform(BoW2N_train_y)\n",
    "BoW2N_valid_y = encoder.fit_transform(BoW2N_valid_y)\n",
    "\n",
    "BoW3N_train_x, BoW3N_valid_x, BoW3N_train_y, BoW3N_valid_y = model_selection.train_test_split(BoW3N_DF['text'], BoW3N_DF['label'])\n",
    "BoW3N_train_y = encoder.fit_transform(BoW3N_train_y)\n",
    "BoW3N_valid_y = encoder.fit_transform(BoW3N_valid_y)\n",
    "\n",
    "tf_train_x, tf_valid_x, tf_train_y, tf_valid_y = model_selection.train_test_split(tf_DF['text'], tf_DF['label'])\n",
    "tf_train_y = encoder.fit_transform(tf_train_y)\n",
    "tf_valid_y = encoder.fit_transform(tf_valid_y)\n",
    "\n",
    "w2v_train_x, w2v_valid_x, w2v_train_y, w2v_valid_y = model_selection.train_test_split(w2v_DF['text'], w2v_DF['label'])\n",
    "w2v_train_y = encoder.fit_transform(w2v_train_y)\n",
    "w2v_valid_y = encoder.fit_transform(w2v_valid_y)\n",
    "\n",
    "tf_w2v_train_x, tf_w2v_valid_x, tf_w2v_train_y, tf_w2v_valid_y = model_selection.train_test_split(tf_w2v_DF['text'], tf_w2v_DF['label'])\n",
    "tf_w2v_train_y = encoder.fit_transform(tf_w2v_train_y)\n",
    "tf_w2v_valid_y = encoder.fit_transform(tf_w2v_valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T08:40:01.220630Z",
     "start_time": "2019-07-03T08:39:34.213Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_model(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net=False):\n",
    "    # Fit the training dataset on the classifier\n",
    "    classifier.fit(feature_vector_train, label)\n",
    "    \n",
    "    # Predict the labels on validation dataset\n",
    "    predictions = classifier.predict(feature_vector_valid)\n",
    "    \n",
    "    if is_neural_net:\n",
    "        predictions = predictions.argmax(axis=-1)\n",
    "    \n",
    "    return metrics.accuracy_score(predictions, valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T08:40:01.222488Z",
     "start_time": "2019-07-03T08:39:34.215Z"
    }
   },
   "outputs": [],
   "source": [
    "# Linear Classifier on Count Vectors\n",
    "accuracy = train_model(linear_model.LogisticRegression(), BoW1N_train_x, BoW1N_train_y, BoW1N_valid_x)\n",
    "print(\"LR, Count Vectors: \", accuracy)\n",
    "\n",
    "# Linear Classifier on Word Level TF IDF Vectors\n",
    "accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print(\"LR, WordLevel TF-IDF: \", accuracy)\n",
    "\n",
    "# Linear Classifier on Ngram Level TF IDF Vectors\n",
    "accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "print(\"LR, N-Gram Vectors: \", accuracy)\n",
    "\n",
    "# Linear Classifier on Character Level TF IDF Vectors\n",
    "accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars)\n",
    "print(\"LR, CharLevel Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "100px",
    "left": "1235px",
    "top": "119px",
    "width": "161.562px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
